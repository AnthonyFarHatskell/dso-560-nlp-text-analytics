{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4 (Due Friday, Nov. 19th, 11:59pm PST)\n",
    "\n",
    "1. Identify **three pairs of documents** in the McDonalds review dataset that have over 0.85 cosine similarity using average token word2vec embeddings from spacy.\n",
    "\n",
    "2. Using the `SMS_test` and `SMS_train` datasets, build a classification model (you can simply use the `sklearn.linear_model.LogisticRegression` model used. Please attempt at least two of the vectorization techniques below:\n",
    "    * `CountVectorization`\n",
    "    * `TfIdfVectorization`\n",
    "    * `word2vec` spacy document-level vectors\n",
    "        * if you pick Count or TfIdf Vectorization -> you MUST reduce dimensionality.\n",
    "\n",
    "Make sure you perform the following:\n",
    "* use train/test split\n",
    "* use proper model evaluation metrics\n",
    "* text preprocessing (regex, stemming/lemmatization, stopword removal, grouping entities, etc.)\n",
    "\n",
    "A discussion of the following:\n",
    "* **What techniques** you tried to improve the performance of your model.\n",
    "* What you would try to do, given more time, that would improve the performance of your model.\n",
    "* Provide an example of two **error cases** - a false positive and a false negative - that your model got wrong, and why the model did not predict the correct answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-20T03:47:05.427050Z",
     "start_time": "2021-11-20T03:46:59.208452Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import sys\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-20T03:47:05.460378Z",
     "start_time": "2021-11-20T03:47:05.430580Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       I'm not a huge mcds lover, but I've been to be...\n",
       "1       Terrible customer service. I came in at 9:30pm...\n",
       "2       First they \"lost\" my order, actually they gave...\n",
       "3       I see I'm not the only one giving 1 star. Only...\n",
       "4       Well, it's McDonald's, so you know what the fo...\n",
       "                              ...                        \n",
       "1520    I enjoyed the part where I repeatedly asked if...\n",
       "1521    Worst McDonalds I've been in in a long time! D...\n",
       "1522    When I am really craving for McDonald's, this ...\n",
       "1523    Two points right out of the gate: 1. Thuggery ...\n",
       "1524    I wanted to grab breakfast one morning before ...\n",
       "Name: review, Length: 1525, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mcd_rev = pd.read_csv('mcdonalds-yelp-negative-reviews.csv', encoding = \"ISO-8859-1\")\n",
    "reviews = mcd_rev['review']\n",
    "reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-20T03:47:32.891528Z",
     "start_time": "2021-11-20T03:47:05.468077Z"
    }
   },
   "outputs": [],
   "source": [
    "helper = reviews.copy()\n",
    "\n",
    "x = []\n",
    "for i in helper:\n",
    "    y = nlp(i)\n",
    "    x.append(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-20T03:48:08.260541Z",
     "start_time": "2021-11-20T03:47:32.895040Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-32628f428ad9>:4: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  value = x[i].similarity(x[j])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1515</th>\n",
       "      <th>1516</th>\n",
       "      <th>1517</th>\n",
       "      <th>1518</th>\n",
       "      <th>1519</th>\n",
       "      <th>1520</th>\n",
       "      <th>1521</th>\n",
       "      <th>1522</th>\n",
       "      <th>1523</th>\n",
       "      <th>1524</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.860516</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.860886</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.860516</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.860886</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 1525 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  0    1    2    3         4    5         6    7         8    9     ... 1515  \\\n",
       "0  NaN  NaN  NaN  NaN       NaN  NaN       NaN  NaN       NaN  NaN  ...  NaN   \n",
       "1  NaN  NaN  NaN  NaN       NaN  NaN       NaN  NaN       NaN  NaN  ...  NaN   \n",
       "2  NaN  NaN  NaN  NaN       NaN  NaN       NaN  NaN       NaN  NaN  ...  NaN   \n",
       "3  NaN  NaN  NaN  NaN       NaN  NaN       NaN  NaN       NaN  NaN  ...  NaN   \n",
       "4  NaN  NaN  NaN  NaN       NaN  NaN  0.860516  NaN  0.860886  NaN  ...  NaN   \n",
       "5  NaN  NaN  NaN  NaN       NaN  NaN       NaN  NaN       NaN  NaN  ...  NaN   \n",
       "6  NaN  NaN  NaN  NaN  0.860516  NaN       NaN  NaN       NaN  NaN  ...  NaN   \n",
       "7  NaN  NaN  NaN  NaN       NaN  NaN       NaN  NaN       NaN  NaN  ...  NaN   \n",
       "8  NaN  NaN  NaN  NaN  0.860886  NaN       NaN  NaN       NaN  NaN  ...  NaN   \n",
       "9  NaN  NaN  NaN  NaN       NaN  NaN       NaN  NaN       NaN  NaN  ...  NaN   \n",
       "\n",
       "  1516 1517 1518 1519 1520 1521 1522 1523 1524  \n",
       "0  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "1  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "2  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "3  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "4  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "5  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "6  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "7  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "8  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "9  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "\n",
       "[10 rows x 1525 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_df = pd.DataFrame(index = range(1525), columns = range(1525))\n",
    "for i in range(1525):\n",
    "    for j in range(1525):\n",
    "        value = x[i].similarity(x[j])\n",
    "        if value < 0.849:\n",
    "            continue\n",
    "        if value == 1:\n",
    "            value = 0\n",
    "        else:\n",
    "            similarity_df.loc[i,j] = value\n",
    "            \n",
    "similarity_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-20T03:49:09.045013Z",
     "start_time": "2021-11-20T03:48:08.263327Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Three documents with cosine similarities above 0.85 are [630, 1271, 221]\n",
      "\n",
      "Document 630 has a cosine similarity of 0.8590865144015573 to Document 1271 and they look like \n",
      "   Document 630: I did it for you...Okay, we've all been there. This morning I decided to try out this new fangled chicken biscuit for breakfast. Not sure what all played into that idea. Maybe it was my craving for McFood after last nights Yelp event at the Dockside. Maybe it was the commercial right before I pulled into the drive through, whatever. At any rate, I did it for you. I tried it so you don't have to. Turns out, the chicken is different. Its moist & the batter is appealing to the eye. But as soon as you get one whiff you know that's some funky chicken. I think maybe they reached a little too far with this one. They tried to be too southern or too something and they screwed it up with some herb or flavor that just shouldn't be there. I think maybe it was dill? Either way, its all wrong. Take my word for it, stick to the sausage on that biscuit.\n",
      "   Document 1271: I come here for the free Wi-Fi.There is Chinese motif for the decor. Never really had a problem with the food. It's the same as any other McDonald. They let you use the bathrooms here and it was decent last 3 times I used it. It was not the dirtiest bathroom I been to. Not the cleanest, not the dirtiest. It's a small McDonald's. Not the tiniest but it definitely doesn't seat many people.\n",
      "\n",
      "Document 630 has a cosine similarity of 0.8571343924975159 to Document 221 and they look like \n",
      "   Document 630: I did it for you...Okay, we've all been there. This morning I decided to try out this new fangled chicken biscuit for breakfast. Not sure what all played into that idea. Maybe it was my craving for McFood after last nights Yelp event at the Dockside. Maybe it was the commercial right before I pulled into the drive through, whatever. At any rate, I did it for you. I tried it so you don't have to. Turns out, the chicken is different. Its moist & the batter is appealing to the eye. But as soon as you get one whiff you know that's some funky chicken. I think maybe they reached a little too far with this one. They tried to be too southern or too something and they screwed it up with some herb or flavor that just shouldn't be there. I think maybe it was dill? Either way, its all wrong. Take my word for it, stick to the sausage on that biscuit.\n",
      "   Document 221: We came here the day after Christmas with our 2 kids and our nephew to quickly eat breakfast. We were greeted in a very non friendly demeanor by this older woman who just couldn't get our order right. Once we got our order, she still got it wrong and I went back and said something to her. No apologies. She would not give me the small coffee I wanted instead of the large coffee and she short changed us a sausage burrito.So, I bought another coffee, even though I should not have had to pay for it because I was in a rush. She just had a really sour attitude and I know she probably doesn't like working there, but it's not my fault. She accepted working there.So, for that, I give her a 1 star.However, the next day, we did come and the younger man was in her place and he was pretty much perfect. Got everything right. Great attitude, no issues. I give him a 5 star.\n",
      "\n",
      "Document 1271 has a cosine similarity of 0.908309605022192 to Document 221 and they look like \n",
      "   Document 1271: I come here for the free Wi-Fi.There is Chinese motif for the decor. Never really had a problem with the food. It's the same as any other McDonald. They let you use the bathrooms here and it was decent last 3 times I used it. It was not the dirtiest bathroom I been to. Not the cleanest, not the dirtiest. It's a small McDonald's. Not the tiniest but it definitely doesn't seat many people.\n",
      "   Document 221: We came here the day after Christmas with our 2 kids and our nephew to quickly eat breakfast. We were greeted in a very non friendly demeanor by this older woman who just couldn't get our order right. Once we got our order, she still got it wrong and I went back and said something to her. No apologies. She would not give me the small coffee I wanted instead of the large coffee and she short changed us a sausage burrito.So, I bought another coffee, even though I should not have had to pay for it because I was in a rush. She just had a really sour attitude and I know she probably doesn't like working there, but it's not my fault. She accepted working there.So, for that, I give her a 1 star.However, the next day, we did come and the younger man was in her place and he was pretty much perfect. Got everything right. Great attitude, no issues. I give him a 5 star.\n"
     ]
    }
   ],
   "source": [
    "# now we must return the first 3 documents that have cosine similarity >0.85\n",
    "\n",
    "#we randomize a little bit since it was taking too long to get the highest value from the pairs\n",
    "np.random.seed(17)\n",
    "random_starts = np.random.randint(1,1500, size = 3)\n",
    "\n",
    "lister = [np.nan, np.nan, np.nan]\n",
    "ret_list = [0, 0, 0]\n",
    "\n",
    "for i in range(random_starts[0],1525):\n",
    "    for j in range(random_starts[1],1525):\n",
    "        for k in range(random_starts[2], 1525):\n",
    "            pair12 = similarity_df.loc[i,j]\n",
    "            pair13 = similarity_df.loc[i,k]\n",
    "            pair23 = similarity_df.loc[j,k]\n",
    "            lister = [pair12, pair13, pair23]\n",
    "            \n",
    "            if np.isnan(lister).sum() == 0:\n",
    "                ret_list = [i, j, k]\n",
    "                break\n",
    "        if np.isnan(lister).sum() == 0:\n",
    "            break\n",
    "    if np.isnan(lister).sum() == 0:\n",
    "        break\n",
    "\n",
    "print(f'Three documents with cosine similarities above 0.85 are {ret_list}\\n') \n",
    "\n",
    "print(f'Document {ret_list[0]} has a cosine similarity of {lister[0]} to Document {ret_list[1]} and \\\n",
    "they look like \\n   Document {ret_list[0]}: {helper[ret_list[0]]}\\n\\\n",
    "   Document {ret_list[1]}: {helper[ret_list[1]]}\\n')\n",
    "\n",
    "print(f'Document {ret_list[0]} has a cosine similarity of {lister[2]} to Document {ret_list[2]} and \\\n",
    "they look like \\n   Document {ret_list[0]}: {helper[ret_list[0]]}\\n\\\n",
    "   Document {ret_list[2]}: {helper[ret_list[2]]}\\n')\n",
    "\n",
    "print(f'Document {ret_list[1]} has a cosine similarity of {lister[1]} to Document {ret_list[2]} and \\\n",
    "they look like \\n   Document {ret_list[1]}: {helper[ret_list[1]]}\\n\\\n",
    "   Document {ret_list[2]}: {helper[ret_list[2]]}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-20T03:50:39.811626Z",
     "start_time": "2021-11-20T03:50:39.792985Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Message_body</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "      <td>Non-Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "      <td>Non-Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "      <td>Non-Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Will ü b going to esplanade fr home?</td>\n",
       "      <td>Non-Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "      <td>Spam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Message_body     Label\n",
       "0                         Rofl. Its true to its name  Non-Spam\n",
       "1  The guy did some bitching but I acted like i'd...  Non-Spam\n",
       "2  Pity, * was in mood for that. So...any other s...  Non-Spam\n",
       "3               Will ü b going to esplanade fr home?  Non-Spam\n",
       "4  This is the 2nd time we have tried 2 contact u...      Spam"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "train = pd.read_csv(\"SMS_train.csv\", encoding = \"ISO-8859-1\")\n",
    "train = train.iloc[:,1:]\n",
    "train_X = train['Message_body']\n",
    "train_y = train['Label']== \"Spam\"\n",
    "\n",
    "test = pd.read_csv(\"SMS_test.csv\", encoding = \"ISO-8859-1\")\n",
    "test = test.iloc[:,1:]\n",
    "test_X = test['Message_body']\n",
    "test_y = test['Label'] == \"Spam\"\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-20T03:50:40.661493Z",
     "start_time": "2021-11-20T03:50:40.452334Z"
    }
   },
   "outputs": [],
   "source": [
    "#regex cleaning, stopword setting\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stopword_list = set(stopwords.words('english') + [\".\", \"!\", \"?\", \",\",\":\", \"''\", \"'s\", \"'\", \"``\", \"(\", \")\", \"-\"])\n",
    "\n",
    "for i in range(len(train_X)):\n",
    "    train_X[i] = re.sub(\"&lt;#&gt\", \"\", train_X[i])\n",
    "    train_X[i] = re.sub(\"&gt;\", \"\", train_X[i])\n",
    "    train_X[i] = re.sub(\"&lt;\", \"\", train_X[i])\n",
    "\n",
    "for i in range(len(test_X)):\n",
    "    test_X[i] = re.sub(\"&lt;#&gt\", \"\", test_X[i])\n",
    "    test_X[i] = re.sub(\"&gt;\", \"\", test_X[i])\n",
    "    test_X[i] = re.sub(\"&lt;\", \"\", test_X[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-20T03:50:41.192061Z",
     "start_time": "2021-11-20T03:50:41.133154Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without dimensionality reduction, on the training set, our model is 0.9687 accurate.\n",
      "With dimensionality reduction, on the training set, our model is 0.954 accurate.\n",
      "\n",
      "With dimensionality reduction, on the testing set, our model is 0.6 accurate.\n",
      "\n",
      "Our AUROC is 0.7475\n",
      "\n",
      "And our confusion matrix looks like:\n",
      " [[49 50]\n",
      " [ 0 26]].\n"
     ]
    }
   ],
   "source": [
    "#count vectorize\n",
    "vectorizer = CountVectorizer(ngram_range = (1,1),\n",
    "                            stop_words = stopword_list, #remove stopwords\n",
    "                            token_pattern='(?u)\\\\b[a-zA-Z][a-zA-Z]+\\\\b',\n",
    "                            max_features = 2000,\n",
    "                            min_df = 0.01,\n",
    "                            binary = True)\n",
    "\n",
    "#train model\n",
    "X_train = vectorizer.fit_transform(train_X)\n",
    "lr1 = LogisticRegression()\n",
    "lr1.fit(X_train, train_y)\n",
    "y_train_pred = lr1.predict(X_train)\n",
    "print(f'Without dimensionality reduction, on the training set, our model is {round(np.mean(y_train_pred == train_y),4)} accurate.')\n",
    "\n",
    "# reduce dimensions\n",
    "svd = TruncatedSVD(n_components = 5)\n",
    "Z = svd.fit_transform(X_train)\n",
    "lr1.fit(Z, train_y)\n",
    "y_red_train_pred = lr1.predict(Z)\n",
    "print(f'With dimensionality reduction, on the training set, our model is {round(np.mean(y_red_train_pred == train_y),4)} accurate.')\n",
    "\n",
    "# now we do on test set\n",
    "X_test = vectorizer.fit_transform(test_X)\n",
    "Z = svd.fit_transform(X_test)\n",
    "y_red_test_pred = lr1.predict(Z)\n",
    "print(f'\\nWith dimensionality reduction, on the testing set, our model is {round(np.mean(y_red_test_pred == test_y),4)} accurate.\\n')\n",
    "\n",
    "print(f'Our AUROC is {round(roc_auc_score(y_red_test_pred, test_y),4)}\\n')\n",
    "\n",
    "print(f'And our confusion matrix looks like:\\n {confusion_matrix(y_red_test_pred, test_y)}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-20T03:50:48.170282Z",
     "start_time": "2021-11-20T03:50:41.706447Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Spacy, our model is 0.976 accurate on our training data\n",
      "Using Spacy, our model is 0.84 accurate on our testing data\n",
      "Our AUROC is 0.8551\n",
      "\n",
      "And our confusion matrix looks like:\n",
      " [[49 20]\n",
      " [ 0 56]].\n"
     ]
    }
   ],
   "source": [
    "# spacy \n",
    "\n",
    "train_helper = train_X.copy()\n",
    "\n",
    "x = []\n",
    "t = []\n",
    "for i in train_helper:\n",
    "    y = nlp(i)\n",
    "    x.append(y) # list of text\n",
    "    t.append(y.vector) # list of associated vectors\n",
    "    \n",
    "lr = LogisticRegression()\n",
    "lr.fit(t,train_y)\n",
    "train_prediction = lr.predict(t)\n",
    "print(f'Using Spacy, our model is {round(np.mean(train_prediction == train_y),4)} accurate on our training data')\n",
    "\n",
    "###################################### testing set ##############\n",
    "test_helper = test_X.copy()\n",
    "u = []\n",
    "z = []\n",
    "for i in test_helper:\n",
    "    p = nlp(i)\n",
    "    u.append(p) # list of text\n",
    "    z.append(p.vector) # list of associated vectors\n",
    "test_prediction = lr.predict(z)\n",
    "\n",
    "print(f'Using Spacy, our model is {round(np.mean(test_prediction == test_y),4)} accurate on our testing data')\n",
    "\n",
    "print(f'Our AUROC is {round(roc_auc_score(test_prediction, test_y),4)}\\n')\n",
    "\n",
    "print(f'And our confusion matrix looks like:\\n {confusion_matrix(test_prediction, test_y)}.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "\n",
    "### CountVectorizer\n",
    "- On our testing set, we achieved 60% (75/125) accuracy with an AUC of 0.7475\n",
    "- Our confusion matrix looked like: \n",
    "\n",
    "|             | Non-Spam    | Spam          |\n",
    "| :---        |    :----:   |          ---: |\n",
    "| Non-Spam    |      49     |      50       |\n",
    "| Spam        |      0      |     26        |\n",
    "\n",
    "- We misclassify about 2/3 of our spam messages as non-spam , i.e. false negative errors\n",
    "\n",
    "### Spacy\n",
    "- On our testing set, we achieved 84% (105/125) accuracy with an AUC of 0.855\n",
    "- Our confusion matrix looked like:\n",
    "\n",
    "|             | Non-Spam    | Spam          |\n",
    "| :---        |    :----:   |          ---: |\n",
    "| Non-Spam    |      49     |      20       |\n",
    "| Spam        |      0      |     56        |\n",
    "\n",
    "- We still make most of our misclassification errors with false negatives, although this model performs better than our CountVectorizer\n",
    "\n",
    "\n",
    "### Other comments\n",
    "- **Techniques used to try and improve our models were:**\n",
    "    - Different variations of ngram ranges for CountVectorizer (bugs with higher n values)\n",
    "    - Adding in binary = True improved model performance\n",
    "    - Adding certain stopwords and regex cleaning\n",
    "    - Performing train/test split before and after vectorizing (same results)\n",
    "- **Given more time, we would:**\n",
    "    - Do more regex cleaning\n",
    "    - Qualitatively assess the reviews to gain insights into additional stopwords to remove\n",
    "    - Perform CV analysis to optimize our regression model\n",
    "- **Two error cases:**\n",
    "    - Our better model (spacy) misclassifies certain spam texts as non-spam (we have no messages tagged as spam when they are not supposed to be). Two examples of this are:\n",
    "        - *Someone has contacted our dating service and entered your phone because they fancy you! To find out who it is call from a landline 09111032124 . PoBox12n146tf150p*\n",
    "        - *Thanks for your Ringtone Order, Reference T91. You will be charged GBP 4 per week. You can unsubscribe at anytime by calling customer services on 09057039994*\n",
    "    - These two are coming up as non-spam instead of spam since they *resemble* regular text messages more than spam texts based on our model. They also *seem* less like spam than a lot of the other messages. The key here is that we must refine our technique and model when training it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
