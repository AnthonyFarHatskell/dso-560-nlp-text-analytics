{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Setup\" data-toc-modified-id=\"Setup-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Setup</a></span></li><li><span><a href=\"#Information-Retrieval\" data-toc-modified-id=\"Information-Retrieval-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Information Retrieval</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Practical-Example\" data-toc-modified-id=\"Practical-Example-2.0.1\"><span class=\"toc-item-num\">2.0.1&nbsp;&nbsp;</span>Practical Example</a></span></li><li><span><a href=\"#Boolean-Search-Queries\" data-toc-modified-id=\"Boolean-Search-Queries-2.0.2\"><span class=\"toc-item-num\">2.0.2&nbsp;&nbsp;</span>Boolean Search Queries</a></span></li><li><span><a href=\"#Examples-of-Documents-That-Would-Satisfy-the-Above-Document-Term-Matrix\" data-toc-modified-id=\"Examples-of-Documents-That-Would-Satisfy-the-Above-Document-Term-Matrix-2.0.3\"><span class=\"toc-item-num\">2.0.3&nbsp;&nbsp;</span>Examples of Documents That Would Satisfy the Above Document-Term Matrix</a></span><ul class=\"toc-item\"><li><span><a href=\"#Query-Example-(nuclear-AND-treaty)\" data-toc-modified-id=\"Query-Example-(nuclear-AND-treaty)-2.0.3.1\"><span class=\"toc-item-num\">2.0.3.1&nbsp;&nbsp;</span>Query Example <code>(nuclear AND treaty)</code></a></span></li><li><span><a href=\"#Query-Example-(nuclear-AND-treaty)-OR-((NOT-treaty)-AND-(nonproliferation-OR-Iran))\" data-toc-modified-id=\"Query-Example-(nuclear-AND-treaty)-OR-((NOT-treaty)-AND-(nonproliferation-OR-Iran))-2.0.3.2\"><span class=\"toc-item-num\">2.0.3.2&nbsp;&nbsp;</span>Query Example <code>(nuclear AND treaty) OR ((NOT treaty) AND (nonproliferation OR Iran))</code></a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Tokenizing-Sentences\" data-toc-modified-id=\"Tokenizing-Sentences-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Tokenizing Sentences</a></span><ul class=\"toc-item\"><li><span><a href=\"#Definition-of-a-Token\" data-toc-modified-id=\"Definition-of-a-Token-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Definition of a Token</a></span></li><li><span><a href=\"#Examples-of-Tokens/Words-Not-Being-the-Identical-Concepts\" data-toc-modified-id=\"Examples-of-Tokens/Words-Not-Being-the-Identical-Concepts-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Examples of Tokens/Words Not Being the Identical Concepts</a></span></li><li><span><a href=\"#Definition-of-a-Document\" data-toc-modified-id=\"Definition-of-a-Document-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Definition of a Document</a></span></li><li><span><a href=\"#Why-Is-Tokenization-Hard?\" data-toc-modified-id=\"Why-Is-Tokenization-Hard?-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Why Is Tokenization Hard?</a></span><ul class=\"toc-item\"><li><span><a href=\"#Sentence-Boundary-Detection\" data-toc-modified-id=\"Sentence-Boundary-Detection-3.4.1\"><span class=\"toc-item-num\">3.4.1&nbsp;&nbsp;</span>Sentence Boundary Detection</a></span></li><li><span><a href=\"#Approaches-to-Sentence-Boundary-Detection\" data-toc-modified-id=\"Approaches-to-Sentence-Boundary-Detection-3.4.2\"><span class=\"toc-item-num\">3.4.2&nbsp;&nbsp;</span>Approaches to Sentence Boundary Detection</a></span><ul class=\"toc-item\"><li><span><a href=\"#Rules-Based\" data-toc-modified-id=\"Rules-Based-3.4.2.1\"><span class=\"toc-item-num\">3.4.2.1&nbsp;&nbsp;</span>Rules-Based</a></span></li><li><span><a href=\"#Supervised/Unsupervised-Learning\" data-toc-modified-id=\"Supervised/Unsupervised-Learning-3.4.2.2\"><span class=\"toc-item-num\">3.4.2.2&nbsp;&nbsp;</span>Supervised/Unsupervised Learning</a></span></li></ul></li><li><span><a href=\"#NLTK's-PunktSentenceTokenizer\" data-toc-modified-id=\"NLTK's-PunktSentenceTokenizer-3.4.3\"><span class=\"toc-item-num\">3.4.3&nbsp;&nbsp;</span>NLTK's <code>PunktSentenceTokenizer</code></a></span></li></ul></li><li><span><a href=\"#Stemming\" data-toc-modified-id=\"Stemming-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>Stemming</a></span></li><li><span><a href=\"#Lemmatization\" data-toc-modified-id=\"Lemmatization-3.6\"><span class=\"toc-item-num\">3.6&nbsp;&nbsp;</span>Lemmatization</a></span><ul class=\"toc-item\"><li><span><a href=\"#Why-Stemming?\" data-toc-modified-id=\"Why-Stemming?-3.6.1\"><span class=\"toc-item-num\">3.6.1&nbsp;&nbsp;</span>Why Stemming?</a></span></li><li><span><a href=\"#Differences-Between-Stemming-and-Lemmatization\" data-toc-modified-id=\"Differences-Between-Stemming-and-Lemmatization-3.6.2\"><span class=\"toc-item-num\">3.6.2&nbsp;&nbsp;</span>Differences Between Stemming and Lemmatization</a></span></li></ul></li><li><span><a href=\"#Scoring-Metrics\" data-toc-modified-id=\"Scoring-Metrics-3.7\"><span class=\"toc-item-num\">3.7&nbsp;&nbsp;</span>Scoring Metrics</a></span><ul class=\"toc-item\"><li><span><a href=\"#Precision/Recall\" data-toc-modified-id=\"Precision/Recall-3.7.1\"><span class=\"toc-item-num\">3.7.1&nbsp;&nbsp;</span>Precision/Recall</a></span></li><li><span><a href=\"#F1-Score\" data-toc-modified-id=\"F1-Score-3.7.2\"><span class=\"toc-item-num\">3.7.2&nbsp;&nbsp;</span>F1 Score</a></span></li><li><span><a href=\"#Exercises\" data-toc-modified-id=\"Exercises-3.7.3\"><span class=\"toc-item-num\">3.7.3&nbsp;&nbsp;</span>Exercises</a></span></li></ul></li><li><span><a href=\"#Removing-Stopwords\" data-toc-modified-id=\"Removing-Stopwords-3.8\"><span class=\"toc-item-num\">3.8&nbsp;&nbsp;</span>Removing Stopwords</a></span><ul class=\"toc-item\"><li><span><a href=\"#CountVectorize-the-Documents\" data-toc-modified-id=\"CountVectorize-the-Documents-3.8.1\"><span class=\"toc-item-num\">3.8.1&nbsp;&nbsp;</span>CountVectorize the Documents</a></span></li><li><span><a href=\"#When-Should-You-Avoid-Removing-Stopwords?\" data-toc-modified-id=\"When-Should-You-Avoid-Removing-Stopwords?-3.8.2\"><span class=\"toc-item-num\">3.8.2&nbsp;&nbsp;</span>When Should You Avoid Removing Stopwords?</a></span></li></ul></li><li><span><a href=\"#Exercise:\" data-toc-modified-id=\"Exercise:-3.9\"><span class=\"toc-item-num\">3.9&nbsp;&nbsp;</span>Exercise:</a></span></li></ul></li><li><span><a href=\"#Vectorization-Techniques\" data-toc-modified-id=\"Vectorization-Techniques-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Vectorization Techniques</a></span><ul class=\"toc-item\"><li><span><a href=\"#Count-Vectorization\" data-toc-modified-id=\"Count-Vectorization-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Count Vectorization</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll need to download/install `nltk`, and then also download several of `nltk`'s modules."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T01:54:50.357356Z",
     "start_time": "2021-10-29T01:54:47.906873Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/anthonyfarhat/opt/anaconda3/lib/python3.8/site-packages (3.5)\n",
      "Requirement already satisfied: tqdm in /Users/anthonyfarhat/opt/anaconda3/lib/python3.8/site-packages (from nltk) (4.47.0)\n",
      "Requirement already satisfied: click in /Users/anthonyfarhat/opt/anaconda3/lib/python3.8/site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: joblib in /Users/anthonyfarhat/opt/anaconda3/lib/python3.8/site-packages (from nltk) (0.16.0)\n",
      "Requirement already satisfied: regex in /Users/anthonyfarhat/opt/anaconda3/lib/python3.8/site-packages (from nltk) (2020.6.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install nltk"
>>>>>>> 49893f1efd88573d5690756d970064fdaa6b5eec
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T01:54:53.306153Z",
     "start_time": "2021-10-29T01:54:53.258294Z"
    }
   },
=======
   "execution_count": 1,
   "metadata": {},
>>>>>>> 49893f1efd88573d5690756d970064fdaa6b5eec
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/anthonyfarhat/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
<<<<<<< HEAD
      "[nltk_data]     /Users/anthonyfarhat/nltk_data...\n",
=======
      "[nltk_data]     /Users/yuchen/nltk_data...\n",
>>>>>>> 49893f1efd88573d5690756d970064fdaa6b5eec
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
<<<<<<< HEAD
     "execution_count": 3,
=======
     "execution_count": 1,
>>>>>>> 49893f1efd88573d5690756d970064fdaa6b5eec
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "#nltk now uses outdated algorithms but everything used to be in nltk 2000-2015 ish\n",
    "\n",
    "nltk.download('punkt') # A popular NLTK sentence tokenizer\n",
    "nltk.download('stopwords') # library of common English stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Information retrieval (IR)** is finding material (usually documents) of an unstructured nature (usually text) that **satisfies an information need** from within large collections (usually stored on computers).\n",
    "\n",
    "### Practical Example\n",
    "\n",
    "A mobile app contains a gallery of about 8,000 pieces of artwork that they'd like to sell to users. Each artwork contains artwork name, artist name, descriptions, and some keyword tags. Th"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boolean Search Queries\n",
    "\n",
    "Example of a **document-term matrix**, from [Northeastern University, Fall 2006 Information Retrieval](http://www.ccs.neu.edu/home/jaa/CSG339.06F/Lectures/boolean.pdf):\n",
    "\n",
    "<img src=\"images/boolean_search_query.png\" alt=\"Different Stemming Techniques\" style=\"width:600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples of Documents That Would Satisfy the Above Document-Term Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Document 1**\n",
    "\n",
    "\"The new cookbook was a huge hit with younger millenials.\"\n",
    "* **Document 2**\n",
    "\n",
    "\"Iran has significant nuclear production capabilities.\"\n",
    "* **Document 3**\n",
    "\n",
    "\"The treaty that ended the American Revolution was signed soon thereafter.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query Example `(nuclear AND treaty)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`D7` would get returned since it is the only document that contains both `nuclear` and `treaty`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query Example `(nuclear AND treaty) OR ((NOT treaty) AND (nonproliferation OR Iran))`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `D7`: satisfies LHS (left-hand side of expression) - contains `nuclear` and `treaty`\n",
    "* `D5`: satisfied RHS - contains `nonproliferation` and not `treaty`)\n",
    "* `D2`: satisfies RHS - contains `Iran` and not `treaty`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing Sentences\n",
    "\n",
    "## Definition of a Token\n",
    "\n",
    "> A token is an instance of a sequence of characters in some particular document that are grouped together as a useful semantic unit for processing. ([Stanford NLP](https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html)):\n",
    "\n",
    "<img src=\"images/tokenization.png\" alt=\"Different Stemming Techniques\" style=\"width:600px;\"/>\n",
    "\n",
    "\n",
    "## Examples of Tokens/Words Not Being the Identical Concepts\n",
    "\n",
    "* Multi-word entities: `San Francisco`, `New York City`\n",
    "* Phone numbers / dates: `(800) 123-4564` and `Apr. 30th, 2020`\n",
    "\n",
    "\n",
    "At first, the task of tokenizing seems simple:\n",
    "1. Split apart `corpus` into `documents`.\n",
    "2. Split apart `documents` into `tokens`.\n",
    "\n",
    "## Definition of a Document\n",
    "\n",
    "A document is a distinct group of tokens. For example, in the Amazon toy product review dataset, each review can be considered a document. In *Tale of Two Cities*, a document could be either a sentence, or a paragraph, or a chapter - it all depends on the task at hand.\n",
    "\n",
    "* **Sentiment Analysis**: each document is a review/comment/tweet\n",
    "* **Authorship Classification** (determining if a piece of text was written by Person A or Person B): the entire text is a document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Is Tokenization Hard?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Boundary Detection\n",
    "\n",
    "Deciding where one sentence ends and another sentence begins is easy for humans, but extremely complex for machines. A prime cause of this is ambiguity of punctuation marks. Particularly in English, periods can mean many things beyond denoting the end of a sentence:\n",
    "* abbreviations\n",
    "* decimal points\n",
    "* domain names and email addresses\n",
    "\n",
    "Furthermore, other \"obvious\" sentence boundaries like question marks and exclamation marks are becoming increasingly ambiguous due to slang, emoticons, or just emphasis (`\"What are you doing???\"` should not be split into 3 separate sentences).\n",
    "\n",
    "### Approaches to Sentence Boundary Detection\n",
    "\n",
    "#### Rules-Based\n",
    "\n",
    "* Using regex: `((?<=[a-z0-9][.?!])|(?<=[a-z0-9][.?!]\\\"))(\\s|\\r\\n)(?=\\\"?[A-Z])` (don't worry if you can't understand this yet- it includes usage of several advanced techniques such as **lookahead/behind references** which we have not covered).\n",
    "\n",
    "#### Supervised/Unsupervised Learning\n",
    "\n",
    "* Maximum entropy models ([UC Berkeley's SATZ Adaptive Sentence Boundary Detector](https://web.archive.org/web/20070922132340/http://elib.cs.berkeley.edu/src/satz/))\n",
    "* NLTK's `Punkt` sentence tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T02:19:42.952750Z",
     "start_time": "2021-10-29T02:19:42.944856Z"
    }
   },
=======
   "execution_count": 9,
   "metadata": {},
>>>>>>> 49893f1efd88573d5690756d970064fdaa6b5eec
   "outputs": [],
   "source": [
    "# Why not just tokenize myself?\n",
    "text = \"I made two purchases today! I bought a bag of grapes for $4.99, \\\n",
    "but then... realized John Francis already bought some at the Y.M.C.A!\""
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T02:19:43.361885Z",
     "start_time": "2021-10-29T02:19:43.354198Z"
    }
   },
=======
   "execution_count": 3,
   "metadata": {},
>>>>>>> 49893f1efd88573d5690756d970064fdaa6b5eec
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I made two purchases today! I bought a bag of grapes for $4',\n",
       " '99, but then',\n",
       " '',\n",
       " '',\n",
       " ' realized John Francis already bought some at the Y',\n",
       " 'M',\n",
       " 'C',\n",
       " 'A!']"
      ]
     },
<<<<<<< HEAD
     "execution_count": 5,
=======
     "execution_count": 3,
>>>>>>> 49893f1efd88573d5690756d970064fdaa6b5eec
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trying to write our own tokenizer\n",
    "text.split(\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are some of the issues you notice with the above approach?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK's `PunktSentenceTokenizer`\n",
    "\n",
    "> `PunktSentenceTokenizer` is an **sentence boundary detection algorithm** that must be trained to be used. NLTK already includes a pre-trained version of the `PunktSentenceTokenizer` ([StackOverflow](https://stackoverflow.com/questions/35275001/use-of-punktsentencetokenizer-in-nltk))"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T02:20:36.339113Z",
     "start_time": "2021-10-29T02:20:36.313776Z"
    }
   },
=======
   "execution_count": 4,
   "metadata": {},
>>>>>>> 49893f1efd88573d5690756d970064fdaa6b5eec
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I made two purchases today!',\n",
       " 'I bought a bag of grapes for $4.99, but then... realized John Francis already bought some at the Y.M.C.A!']"
      ]
     },
<<<<<<< HEAD
     "execution_count": 6,
=======
     "execution_count": 4,
>>>>>>> 49893f1efd88573d5690756d970064fdaa6b5eec
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using NLTK sent_tokenize()\n",
    "sent_text = nltk.sent_tokenize(text) # this gives us a list of sentences\n",
    "sent_text"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T03:16:07.279292Z",
     "start_time": "2021-10-29T03:16:07.269785Z"
    }
   },
=======
   "execution_count": 10,
   "metadata": {},
>>>>>>> 49893f1efd88573d5690756d970064fdaa6b5eec
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'made',\n",
       " 'two',\n",
       " 'purchases',\n",
       " 'today',\n",
       " '!',\n",
       " 'I',\n",
       " 'bought',\n",
<<<<<<< HEAD
       " 'bag',\n",
       " 'grapes',\n",
       " '$',\n",
       " '4.99',\n",
       " ',',\n",
=======
       " 'a',\n",
       " 'bag',\n",
       " 'of',\n",
       " 'grapes',\n",
       " 'for',\n",
       " '$',\n",
       " '4.99',\n",
       " ',',\n",
       " 'but',\n",
       " 'then',\n",
>>>>>>> 49893f1efd88573d5690756d970064fdaa6b5eec
       " '...',\n",
       " 'realized',\n",
       " 'John',\n",
       " 'Francis',\n",
       " 'already',\n",
       " 'bought',\n",
<<<<<<< HEAD
=======
       " 'some',\n",
       " 'at',\n",
       " 'the',\n",
>>>>>>> 49893f1efd88573d5690756d970064fdaa6b5eec
       " 'Y.M.C.A',\n",
       " '!']"
      ]
     },
<<<<<<< HEAD
     "execution_count": 15,
=======
     "execution_count": 10,
>>>>>>> 49893f1efd88573d5690756d970064fdaa6b5eec
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(text)\n",
<<<<<<< HEAD
    "stopword_removed = [token for token in tokens if token not in stopwords]\n",
    "stopword_removed"
=======
    "stopword_removed = [token for token in tokens if token not in stopwords]"
>>>>>>> 49893f1efd88573d5690756d970064fdaa6b5eec
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "\n",
    "Stemming is the process of reducing inflection in words to their root forms such as mapping a group of words to the same stem even if the stem itself is not a valid word in the language [Source](https://www.datacamp.com/community/tutorials/stemming-lemmatization-python)\n",
    "\n",
    "<img src=\"images/stemming-examples.png\" alt=\"Different Stemming Techniques\" style=\"width:600px;\"/>\n",
    "\n",
    "In Python, we can use **`nltk.stem.porter.PorterStemmer`** stem our words:\n",
    "\n",
    "```python\n",
    "stemmer = PorterStemmer()\n",
    "print(stemmer.stem(\"caressed\"))  # caress\n",
    "print(stemmer.stem(\"athlete\"))  # athlet\n",
    "print(stemmer.stem(\"athletics\"))  # athlet\n",
    "print(stemmer.stem(\"media\"))  # media\n",
    "print(stemmer.stem(\"photography\"))  # photographi\n",
    "print(stemmer.stem(\"sexy\"))  # sexi\n",
    "print(stemmer.stem(\"journalling\"))  # journal\n",
    "print(stemmer.stem(\"Slovakia\")) # slovakia\n",
    "print(stemmer.stem(\"corpora\")) # corpora\n",
    "print(stemmer.stem(\"thieves\")) # thiev\n",
    "print(stemmer.stem(\"rocks\")) # rock\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "\n",
    "Lemmatization is closely related to stemming. However, while stemming looks only at the individual word itself, and considers the usage of the word (ie. part of speech, is this word a noun, a verb, etc.). For example, if we compared the [outputs of stemming and lemmatization certain ambiguous French words](https://blog.bitext.com/lemmatization-vs-stemming):\n",
    "\n",
    "<img src=\"images/comparison.png\" alt=\"Different Stemming Techniques\" style=\"width:600px;\"/>\n",
    "\n",
    "```python\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize(\"caressed\")) #caressed\n",
    "print(lemmatizer.lemmatize(\"athlete\")) #athlete\n",
    "print(lemmatizer.lemmatize(\"athletics\")) #athletics\n",
    "print(lemmatizer.lemmatize(\"media\"))\n",
    "print(lemmatizer.lemmatize(\"photography\")) #photography\n",
    "print(lemmatizer.lemmatize(\"sexy\")) #sexy\n",
    "print(lemmatizer.lemmatize(\"journalling\")) #journalling\n",
    "print(lemmatizer.lemmatize(\"Slovakia\")) #Slovakia\n",
    "print(lemmatizer.lemmatize(\"corpora\")) # corpus\n",
    "print(lemmatizer.lemmatize(\"thieves\")) # thief\n",
    "print(lemmatizer.lemmatize(\"rocks\")) #rock\n",
    "```\n",
    "\n",
    "### Why Stemming?\n",
    "- smaller and faster\n",
    "- simplicity in \"good enough\"\n",
    "- can often **provide higher recall (coverage)** if you are using it for text searching: `drives` and `drivers` will likely shorten to `driv`, which may be useful if your search engine wants to make sure to get all relevant documents, even at the cost of surfacing a few irrelevant documents\n",
    "- could potentially be more useful for predictive models that tend to overfit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differences Between Stemming and Lemmatization\n",
    "> **Stemming** usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes. **Lemmatization** usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma . If confronted with the token `saw`, stemming might return just `s`, whereas lemmatization would attempt to return either `see` or `saw` depending on whether the use of the token was as a verb or a noun. ([Stanford NLP Group](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring Metrics\n",
    "\n",
    "<img src=\"images/confusion_matrix2.png\" alt=\"Different Stemming Techniques\" style=\"width:600px;\"/>\n",
    "\n",
    "### Precision/Recall\n",
    "\n",
    "**Recall:** What percent of the positive classes did the model successfully predict?\n",
    "**Precision:** When a model predicted a positive class, what percentage of the time was it correct?\n",
    "\n",
    "In terms of NLP / stemming / lemmatization:\n",
    "\n",
    "**Recall**: After processing (tokenizing, stemming/lemmatizing) the data, what percent of the relevant search results were surfaced? Ie. - when a user searches for `blue jeans`, did all the results returned include all the relevant items (blue-ish colored denim pants)?\n",
    "\n",
    "**Precision**: After processing (tokenizing, stemming/lemmatizing) the data, what percent of the results returned were relevant?\n",
    "\n",
    "<img src=\"images/matrix_practice3.png\" alt=\"Different Stemming Techniques\" style=\"width:600px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1 Score\n",
    "\n",
    "The F1 score of a model represents the harmonic mean between precision and recall, and is defined as \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "F_{1} = 2 * \\frac{P * R}{P + R}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "An F1 score is often a good measure \n",
    "* our dataset target is class imbalanced (ie. 96% positive, 4% negative)\n",
    "* when we want to balance optimizing for both precision and recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "### Exercises\n",
    "\n",
    "Calculate:\n",
    "\n",
    "1. Overall **accuracy**: $\\frac{?}{?}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "If $C$ is the number of correct predictions, and $N$ is the number of total samples:\n",
    "\\begin{equation}\n",
    "a = \\frac{C}{N}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "a = \\frac{30}{36}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "a = 0.833\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "2. **Precision:** $\\frac{?}{?}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "If $TP$ is the number of true positives, and $FP$ is the number of false positives:\n",
    "\\begin{equation}\n",
    "P = \\frac{TP}{TP + FP}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "P = \\frac{10}{10 + 2}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "P = 0.833\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "3. **Recall:** $\\frac{?}{?}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "If $TP$ is the number of true positives, and $FN$ is the number of false negatives:\n",
    "\\begin{equation}\n",
    "R = \\frac{TP}{TP + FN}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "R = \\frac{10}{10 + 4}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "R = 0.714\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "4. **F1 Score**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "\\begin{equation}\n",
    "F_{1} = 2 * \\frac{P * R}{P + R}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "F_{1} = 2 * \\frac{0.833 * 0.714}{0.833 + 0.714}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "F_{1} =0.769\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Stopwords\n",
    "\n",
    "It's your call ultimately if you want to remove stopwords. There are advantages and disadvantages to both approaches. You will first need to run `nltk.download(\"stopwords\")` to download the set of stopwords for NLTK:"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T03:22:57.749088Z",
     "start_time": "2021-10-29T03:22:57.743476Z"
    }
   },
=======
   "execution_count": 6,
   "metadata": {},
>>>>>>> 49893f1efd88573d5690756d970064fdaa6b5eec
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "{'doesn', 'very', 'yourselves', 'hadn', 'who', 'wouldn', 'their', 'her', 'him', 'not', 'under', 'won', 'then', \"you'd\", 'both', 'themselves', 'm', \"hadn't\", \"shan't\", 'with', 'itself', 'only', \"she's\", 'just', 'some', 'y', 'of', 'out', 'too', 'than', 't', 'this', 'weren', 'once', 'yours', 'further', 'will', 'there', 'what', 'up', 'own', 're', \"wouldn't\", 'a', 'o', 'more', 'down', 'herself', 'about', 'all', 'now', 'no', \"won't\", 'you', 'do', 'through', 'haven', 'ourselves', 've', 'when', 'while', 'himself', 'and', 'here', 'such', 'same', \"you've\", \"aren't\", 'that', 'didn', 'or', 'had', 'did', 'ain', 'does', 'after', 'shan', 'shouldn', \"haven't\", 'mightn', 'where', 'were', \"didn't\", 'these', 'before', 'below', 'having', \"you'll\", 'whom', \"doesn't\", 'nor', 'theirs', 'between', 'into', 'she', 'any', 'is', 'should', 'couldn', 'why', \"shouldn't\", 'it', 'during', 'which', 'am', \"isn't\", 'to', 'for', 'by', 'against', 'your', 'ma', 'off', \"hasn't\", 'over', 'an', 'but', 'few', 'myself', 'hers', 'needn', 'from', 'they', \"mightn't\", \"you're\", 'have', 'again', 'he', 's', 'ours', 'as', 'been', 'them', 'yourself', 'other', 'me', 'the', 'until', 'at', \"wasn't\", 'on', \"that'll\", 'are', \"it's\", 'being', 'our', 'can', 'how', 'be', 'we', 'isn', \"needn't\", 'its', 'his', 'so', \"couldn't\", 'i', \"should've\", 'each', 'd', 'hasn', 'has', 'll', 'my', 'doing', \"mustn't\", 'was', 'if', \"weren't\", 'most', 'wasn', 'because', 'don', 'those', 'mustn', 'in', 'above', 'aren', \"don't\"}\n"
=======
      "{'any', 'a', 'you', \"didn't\", 'more', 'does', 'myself', 'm', \"couldn't\", 'am', 's', 'what', 'can', 'between', 'all', 'wasn', 'aren', 'ours', 'hasn', 've', 'how', 'there', 'didn', 'shouldn', 'needn', 'ain', 'above', 'they', 'are', \"shan't\", \"won't\", 'same', 'for', 'she', 'our', 'have', 'before', 'so', 'doesn', 'with', 't', 'me', 'the', 'here', 'under', 'after', 'off', 'through', 'hers', 'each', 'hadn', 'ma', 'because', \"don't\", 'until', 'him', 'over', 'should', 'to', \"isn't\", 'very', 'wouldn', 'were', 'will', 'on', 'shan', 'nor', 'about', 'too', 'is', 'in', 'whom', 'i', 'll', 'been', 'had', 'at', 'them', 'again', \"it's\", 'of', 'my', 'into', 'yourselves', 'these', 'its', 'having', 'not', 'who', 'those', 'be', 'while', 'being', \"you'll\", 'down', 'd', 'other', \"wasn't\", 'both', 'that', 'did', \"hadn't\", 'has', 'or', \"doesn't\", 'itself', 'an', 'which', 'theirs', 'once', 'such', \"should've\", \"weren't\", \"that'll\", \"you'd\", 're', 'won', \"mightn't\", 'haven', 'most', 'then', \"hasn't\", 'mightn', 'weren', 'mustn', 'doing', 'from', 'some', 'y', \"you're\", 'yourself', 'it', 'further', 'ourselves', \"needn't\", \"mustn't\", 'we', 'couldn', 'themselves', \"she's\", 'your', 'now', 'below', 'up', \"haven't\", 'himself', 'out', 'this', 'just', 'during', 'and', 'isn', \"shouldn't\", 'where', \"aren't\", \"you've\", 'no', 'yours', 'don', 'by', 'his', 'do', 'was', 'he', 'but', 'only', 'why', 'as', 'against', 'their', 'when', 'o', 'few', 'than', 'her', 'own', 'herself', \"wouldn't\", 'if'}\n"
>>>>>>> 49893f1efd88573d5690756d970064fdaa6b5eec
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(set(stopwords.words('english'))) # see the set of words NLTK considers stopwords"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T03:21:38.799786Z",
     "start_time": "2021-10-29T03:21:38.791736Z"
    }
   },
=======
   "execution_count": 12,
   "metadata": {},
>>>>>>> 49893f1efd88573d5690756d970064fdaa6b5eec
   "outputs": [
    {
     "data": {
      "text/plain": [
<<<<<<< HEAD
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 31,
=======
       "True"
      ]
     },
     "execution_count": 12,
>>>>>>> 49893f1efd88573d5690756d970064fdaa6b5eec
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
<<<<<<< HEAD
   "source": [
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T03:24:39.554021Z",
     "start_time": "2021-10-29T03:24:39.548456Z"
    }
   },
   "outputs": [],
=======
>>>>>>> 49893f1efd88573d5690756d970064fdaa6b5eec
   "source": [
    "stopwords = set(stopwords.words('english'))\n",
    "\"the\" in stopwords"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T03:24:40.023095Z",
     "start_time": "2021-10-29T03:24:39.998981Z"
    }
   },
=======
   "execution_count": 14,
   "metadata": {},
>>>>>>> 49893f1efd88573d5690756d970064fdaa6b5eec
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import in a dataset of baltimore's public art galleries\n",
    "public_art_df: pd.DataFrame = pd.read_csv(\"baltimore_public_art.csv\")\n",
    "\n",
    "public_art_df = public_art_df.replace(np.nan, '', regex=True)\n",
    "public_art_df.head()\n",
    "# use the titleOfArtwork field\n",
    "titles_of_artworks: pd.Series = public_art_df[\"titleOfArtwork\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T03:26:14.980152Z",
     "start_time": "2021-10-29T03:26:14.959993Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Family Group',\n",
       " 'Moses and Christ Reliefs',\n",
       " 'William Donald Schaefer',\n",
       " 'Atlantic Blue Roller Column or Interlocking Piece ',\n",
       " 'Baltimore Pylon',\n",
       " 'Single Form',\n",
       " '',\n",
       " 'Three Baltimores',\n",
       " 'Mother and Child',\n",
       " 'Series of Points on a Line',\n",
       " 'Unknown',\n",
       " 'Angel of Truth',\n",
       " 'Unknown',\n",
       " 'Unknown',\n",
       " 'Unknown',\n",
       " 'Apollo Belvedere',\n",
       " 'Harpie',\n",
       " 'Eagles',\n",
       " 'War Memorial',\n",
       " 'Unknown',\n",
       " \"Serviceman's Memorial\",\n",
       " 'Easy Landing',\n",
       " 'Baltimore City Fire Fighters Monument',\n",
       " 'Relay',\n",
       " '',\n",
       " 'Untitled',\n",
       " 'Fire Chariot',\n",
       " 'Lions',\n",
       " 'Federal Office Building Entrance Fountains',\n",
       " 'Palace Lions',\n",
       " 'Angel of Truth',\n",
       " 'Tauromachy',\n",
       " 'Untitled',\n",
       " 'The Spirit of Constant Care',\n",
       " 'McKeldin Square and Cascade',\n",
       " 'Unknown',\n",
       " '',\n",
       " 'Bad Baby',\n",
       " 'Untitled',\n",
       " \"Serviceman's Memorial\",\n",
       " 'Unknown',\n",
       " 'Untitled',\n",
       " 'Unknown',\n",
       " 'Prometheus Bound II',\n",
       " 'Three Otters',\n",
       " 'Love the Animals',\n",
       " 'Unknown',\n",
       " 'Untitled',\n",
       " 'Unknown',\n",
       " 'Unknown',\n",
       " 'Unknown',\n",
       " 'Unknown',\n",
       " 'Unknown',\n",
       " 'Runner',\n",
       " 'The Nest',\n",
       " '',\n",
       " 'Unknown',\n",
       " '',\n",
       " 'Unknown',\n",
       " 'Medal of Honor',\n",
       " 'Unknown',\n",
       " 'Sidney Lake',\n",
       " \"Mother's Garden\",\n",
       " 'Watson Monument',\n",
       " 'Horse',\n",
       " 'The Visitation',\n",
       " 'Number 10',\n",
       " 'Fifth Regiment Armory Tablet',\n",
       " \"Noah's Ark\",\n",
       " 'Unknown',\n",
       " '',\n",
       " 'Untitled',\n",
       " 'Star Spangled Banner',\n",
       " 'Untitled',\n",
       " 'Colors in Harmony',\n",
       " 'J. Jefferson Miller Fountain',\n",
       " '',\n",
       " 'Unknown',\n",
       " '',\n",
       " '',\n",
       " 'The Young Seated Mercury',\n",
       " 'Unknown',\n",
       " '',\n",
       " 'Bear and Cubs',\n",
       " 'Unknown',\n",
       " 'Little Wild Flower',\n",
       " 'Cannonball and Rack',\n",
       " 'Spanish War Veterans Memorial [The Rifleman/Hiker]',\n",
       " 'Unknown',\n",
       " '',\n",
       " 'The Naiad',\n",
       " 'Sea Urchin (enlargement of original by Edward Berg',\n",
       " 'Veronica',\n",
       " 'African-American Leaders Portraits',\n",
       " 'Unknown',\n",
       " '',\n",
       " 'Unknown',\n",
       " '',\n",
       " 'Veterans of Foreign Wars Monument',\n",
       " '# 31',\n",
       " '',\n",
       " '',\n",
       " 'Unknown',\n",
       " '',\n",
       " 'Discus Thrower',\n",
       " 'Unknown',\n",
       " '',\n",
       " 'Mother and Child II',\n",
       " 'Unknown',\n",
       " 'Untitled',\n",
       " 'Bird in Tree',\n",
       " 'Battery Cannons',\n",
       " 'Colonel George Armistead Monument',\n",
       " 'Dreams',\n",
       " 'Grove of Rememberance',\n",
       " 'Jacob France Memorial Fountain',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'Saint-Lo Drive Memorial',\n",
       " 'Unknown',\n",
       " 'Westside',\n",
       " 'Unknown',\n",
       " 'Unknown',\n",
       " 'Unknown',\n",
       " 'Statue of Liberty (copy)',\n",
       " 'Unknown',\n",
       " 'Unknown',\n",
       " 'Unknown (Clock Relief)',\n",
       " '',\n",
       " 'Isaiah Bowman',\n",
       " '',\n",
       " 'Seen from a Wall in the Memory',\n",
       " '',\n",
       " 'Triaxial Link',\n",
       " 'Unknown',\n",
       " 'Young Girl on a Chair',\n",
       " 'Unknown',\n",
       " 'Washington Monument',\n",
       " 'Paestum',\n",
       " 'Unknown',\n",
       " 'Unknown',\n",
       " 'Snail',\n",
       " '',\n",
       " 'Unknown',\n",
       " 'And Still We Rise',\n",
       " 'Unknown',\n",
       " '',\n",
       " 'Engine Company 43',\n",
       " 'Lakeland Playfield',\n",
       " '',\n",
       " '',\n",
       " 'Untitled',\n",
       " 'Unknown',\n",
       " 'Yuai',\n",
       " 'Birth of Venus',\n",
       " '',\n",
       " 'Unknown',\n",
       " 'Unknown',\n",
       " \"Vietnam Veteran's Memorial Monument\",\n",
       " 'Unknown',\n",
       " 'Ursa Major and Ursa Minor',\n",
       " '',\n",
       " 'Untitled',\n",
       " 'The Vine Covered Pasarelle',\n",
       " '',\n",
       " 'Market Fountain',\n",
       " 'Give Peace A Chance',\n",
       " '',\n",
       " 'Unknown',\n",
       " 'Unknown',\n",
       " 'Unknown',\n",
       " \"Cook's Memorial\",\n",
       " '',\n",
       " 'Edgar Allan Poe',\n",
       " 'Holocaust Memorial',\n",
       " 'OM',\n",
       " 'Unknown',\n",
       " 'Chorale',\n",
       " 'Unknown',\n",
       " 'Unknown',\n",
       " 'Unknown',\n",
       " '',\n",
       " 'Unknown',\n",
       " 'Bluebird of Happiness',\n",
       " 'The Flame at the Holocaust Memorial',\n",
       " 'Elephant',\n",
       " '',\n",
       " 'Titan',\n",
       " 'Saint Lo Monument',\n",
       " 'Cat',\n",
       " 'Poe Monument',\n",
       " 'George Washington',\n",
       " 'Cylburn Lions',\n",
       " 'Unknown',\n",
       " 'Boy With Fish',\n",
       " 'Unknown',\n",
       " 'Unknown',\n",
       " 'Harbor Sound Bell',\n",
       " 'Solar Totum IV',\n",
       " '',\n",
       " 'Sidney Lanier Monument',\n",
       " 'Unknown',\n",
       " 'Conception',\n",
       " '',\n",
       " '',\n",
       " 'Four Dishes',\n",
       " '',\n",
       " 'Maryland Line Monument',\n",
       " '',\n",
       " 'Untitled',\n",
       " 'Untitled \"Turning Point\"',\n",
       " '',\n",
       " '',\n",
       " \"BMA Relief - 'To the Fine Arts'\",\n",
       " '',\n",
       " 'Unknown',\n",
       " 'The Butterflies',\n",
       " 'Untitled',\n",
       " '',\n",
       " 'U.S. Spanish-American War Memorial (\"The Hiker\")',\n",
       " 'Colossus I (fountain)',\n",
       " 'Unknown',\n",
       " 'Freedom Park',\n",
       " 'Veterans Monument',\n",
       " 'Sails',\n",
       " '',\n",
       " 'Unknown',\n",
       " 'Untitled',\n",
       " 'Soying Seeds of Sandtown',\n",
       " 'Sundial',\n",
       " 'World War II Memorial Shaft',\n",
       " '',\n",
       " '',\n",
       " 'Baltimore Passage, Albermarle Squre',\n",
       " 'Bank Job',\n",
       " 'Unknown',\n",
       " 'Reese Monument',\n",
       " 'Unknown',\n",
       " 'Latrobe Monument',\n",
       " 'Unknown',\n",
       " 'Unknown',\n",
       " 'Fell Family Memorial',\n",
       " 'Unknown',\n",
       " 'A Bird Flying North',\n",
       " '',\n",
       " 'Unknown',\n",
       " '',\n",
       " 'On the Trail',\n",
       " '',\n",
       " \"Serviceman's Memorial\",\n",
       " 'Unknown',\n",
       " 'Unknown',\n",
       " '',\n",
       " 'Unknown - Reliefs Cathedral of Mary our Queen',\n",
       " 'Wallace Monument',\n",
       " 'Baltimore',\n",
       " '',\n",
       " 'Unknown',\n",
       " '',\n",
       " '',\n",
       " 'Unknown',\n",
       " 'Oasis',\n",
       " 'turtle',\n",
       " 'Unknown (\"The Question Mark\"',\n",
       " 'Uni I',\n",
       " 'Colonel George Armistead',\n",
       " 'Unknown',\n",
       " \"Serviceman's Memorial\",\n",
       " 'William H. Welch',\n",
       " 'Unknown',\n",
       " 'Sea Urchin',\n",
       " 'Mr. Sun and Mr. Moon',\n",
       " 'Lions',\n",
       " 'Simon Bolivar Monument',\n",
       " 'Columbus Obelisk',\n",
       " 'Unknown',\n",
       " 'Untitled',\n",
       " 'Cold Spring Outcrop',\n",
       " 'Korean War Memorial',\n",
       " 'Unknown',\n",
       " 'Of Course Culture Horse',\n",
       " 'Unknown',\n",
       " 'Box-headed Figure',\n",
       " 'Untitled',\n",
       " 'Unknown',\n",
       " 'Untitled wall',\n",
       " 'Unknown',\n",
       " '',\n",
       " 'Unknown',\n",
       " 'Boy Releasing Bird',\n",
       " 'Ceramic monlith',\n",
       " 'Unknown',\n",
       " 'Edward J. Gallagher Memorial',\n",
       " 'Christopher Columbus',\n",
       " 'The Rescue',\n",
       " '',\n",
       " 'Uno y Dos',\n",
       " '',\n",
       " 'sun dial',\n",
       " 'Untitled',\n",
       " 'Unknown',\n",
       " 'Memorial Stadium Fa√ßade',\n",
       " 'Repeal',\n",
       " 'Double Ganut',\n",
       " 'Unknown',\n",
       " 'Untitled',\n",
       " 'Unknown',\n",
       " 'Unknown',\n",
       " 'Unknown',\n",
       " 'Patapsco River Project',\n",
       " 'Confederate Women Monument',\n",
       " 'Rogers Bastion',\n",
       " '',\n",
       " 'Unknown',\n",
       " 'The Contortionist',\n",
       " 'Unknown',\n",
       " '',\n",
       " 'Unknown',\n",
       " 'Francis Scott Key:Orpheus',\n",
       " 'Untitled',\n",
       " 'Pearlstone Park',\n",
       " 'A Little Help from Our Friends',\n",
       " 'Unknown',\n",
       " 'Unknown',\n",
       " 'Cecilius Calvert Statue',\n",
       " 'Unknown',\n",
       " 'Lady Baltimore',\n",
       " 'Unknown',\n",
       " 'Unknown',\n",
       " 'giraffe',\n",
       " 'Standing Woman (Elevation)',\n",
       " 'Conradin Kreutzer Monument',\n",
       " 'James Cardinal Gibbons',\n",
       " 'Hopkins monument',\n",
       " 'Unknown',\n",
       " 'Unknown',\n",
       " 'Baltimore Police Department',\n",
       " \"The What Knot - 'Linear Growth Structure'\",\n",
       " 'Unknown',\n",
       " 'Unknown',\n",
       " 'Shelter',\n",
       " 'The Forum Fountain',\n",
       " 'Unknown',\n",
       " 'Bulletin Board',\n",
       " \"My Sister's Garden\",\n",
       " 'The Divine Healer',\n",
       " 'Frederick Douglass',\n",
       " '',\n",
       " 'BELIEVE',\n",
       " 'Unknown',\n",
       " 'Human Will and Energy',\n",
       " 'Unknown',\n",
       " 'Unknown',\n",
       " 'Untitled',\n",
       " '',\n",
       " 'J. Joseph Curran Memorial Clock',\n",
       " 'James L. Ridgely Monument',\n",
       " 'The Learning Tree',\n",
       " '',\n",
       " 'The Prophet(St. John the Baptist)',\n",
       " 'Unknown',\n",
       " '',\n",
       " 'Mercury and Commerce',\n",
       " 'Thurgood Marshall',\n",
       " 'Red Buoyant',\n",
       " 'H.L. Mencken Fountain',\n",
       " 'Unknown',\n",
       " 'Pulaski Monument',\n",
       " 'On Point',\n",
       " '',\n",
       " 'various sculptures',\n",
       " 'Lee-Jackson Monument',\n",
       " 'Nut and Bolt',\n",
       " 'The Arts',\n",
       " 'Untitled',\n",
       " '',\n",
       " 'World WarI Memorial',\n",
       " 'The Family',\n",
       " 'Local #420',\n",
       " 'Unknown',\n",
       " 'Untitled',\n",
       " 'Leaden Hall Tribute',\n",
       " 'Subvator',\n",
       " '',\n",
       " '',\n",
       " 'Father and Children',\n",
       " 'Unknown',\n",
       " '',\n",
       " '',\n",
       " 'Unknown',\n",
       " 'unknown',\n",
       " 'Unknown',\n",
       " 'Wishbone House',\n",
       " 'Unknown',\n",
       " '',\n",
       " \"Ryan's Bird Sanctuary\",\n",
       " 'Kinship',\n",
       " 'Large Bather',\n",
       " 'Unknown',\n",
       " 'Reawakening',\n",
       " 'William T. Walters',\n",
       " 'Wildflower',\n",
       " 'Star Chamber',\n",
       " 'Unknown',\n",
       " 'Recreation and Education',\n",
       " '',\n",
       " 'Brooks Robinson',\n",
       " 'Joy',\n",
       " 'Untitled',\n",
       " 'Unknown',\n",
       " 'Fanned Area',\n",
       " '',\n",
       " 'Sailor and Canton',\n",
       " 'Unknown',\n",
       " 'Boy Strangling a Goose',\n",
       " 'Unknown',\n",
       " 'Untitled',\n",
       " 'Unknown',\n",
       " 'Lions',\n",
       " 'Mythological Boat',\n",
       " 'Yum Yum Tree',\n",
       " 'Unknown',\n",
       " 'Peale Museum Bas Relief',\n",
       " 'Unknown',\n",
       " 'Unknown',\n",
       " '',\n",
       " 'Unknown',\n",
       " 'Union Soldiers and Sailors',\n",
       " \"Dhontshihgwa'es (Creator's Game)\",\n",
       " 'Unknown (Old Post Office Relief)',\n",
       " 'Baltimore Gas and Electric Building frieze',\n",
       " 'Unknown',\n",
       " '',\n",
       " '',\n",
       " 'Columbus Monument',\n",
       " \"Serviceman's Memorial\",\n",
       " 'Sisyphus',\n",
       " 'Unknown',\n",
       " 'Unknown',\n",
       " 'Unknown',\n",
       " 'Knowledge',\n",
       " '',\n",
       " \"Serviceman's Memorial\",\n",
       " 'Fireman Saving a Child',\n",
       " 'Man Walking',\n",
       " 'Biennial exhibition',\n",
       " 'Unknown',\n",
       " 'Untitled',\n",
       " 'Host of the Ellipse',\n",
       " 'Roger B. Taney',\n",
       " 'Tachigata: Lantern and Garden Stones',\n",
       " 'Untitled',\n",
       " 'Unknown',\n",
       " '',\n",
       " 'Unknown',\n",
       " 'Venter',\n",
       " 'Three-Piece Reclining Figure No. 1',\n",
       " 'Unknown',\n",
       " 'Baltimore Musem Of Art Lions',\n",
       " '',\n",
       " '',\n",
       " 'Dromedary Camel',\n",
       " 'Thomas Wildey Monument',\n",
       " 'The Unity Wall',\n",
       " 'Caterpillar',\n",
       " 'The Human Dance',\n",
       " '',\n",
       " 'Unknown',\n",
       " 'Number 9',\n",
       " 'Unknown',\n",
       " 'Peace',\n",
       " '',\n",
       " 'Unknown',\n",
       " 'Unknown',\n",
       " 'Unknown',\n",
       " 'Two Friends',\n",
       " 'Johns Hopkins Monument',\n",
       " 'Unknown',\n",
       " '',\n",
       " 'Unknown',\n",
       " 'Frederick Douglass and the African-American Strugg',\n",
       " 'Seal',\n",
       " 'Daniel Coit Gilman',\n",
       " 'Fruit',\n",
       " 'Unknown',\n",
       " \"Captain John O'Donnell\",\n",
       " '',\n",
       " 'St. Francis',\n",
       " 'Unknown',\n",
       " 'Unknown',\n",
       " '',\n",
       " '',\n",
       " 'Unknown',\n",
       " 'Samurai Rocker',\n",
       " 'Steel Henge',\n",
       " 'Untitled',\n",
       " '',\n",
       " 'suspended aircraft 1890-1912',\n",
       " 'courthouse plaza fountain',\n",
       " 'Unknown',\n",
       " '',\n",
       " '',\n",
       " 'Owl',\n",
       " 'The Seasons',\n",
       " 'Francis Scott Key Monument',\n",
       " 'Unknown',\n",
       " '',\n",
       " 'Centennial Sculpture',\n",
       " \"Penguin's Prayer\",\n",
       " 'Unknown',\n",
       " \"Babe's Dream\",\n",
       " '',\n",
       " '',\n",
       " 'Untitled',\n",
       " 'Pegasus',\n",
       " 'Unknown',\n",
       " 'Untitled playground',\n",
       " '',\n",
       " '',\n",
       " 'Granite Memorial Shaft',\n",
       " 'The Quest',\n",
       " 'The Miracle',\n",
       " 'Unknown',\n",
       " 'Unknown',\n",
       " 'Bactrian Camel',\n",
       " 'New Song',\n",
       " '',\n",
       " 'Fallsway Fountain',\n",
       " 'Maryland Line Monument',\n",
       " 'Chapin A. Harris bust',\n",
       " '\"Calvert Street Bridge\" Lions (3)',\n",
       " 'John Eager Howard',\n",
       " 'Unknown',\n",
       " '',\n",
       " 'Unknown',\n",
       " 'Cultura',\n",
       " 'Brio',\n",
       " 'Archimedean Spiral',\n",
       " 'Unknown',\n",
       " 'Unknown',\n",
       " 'Confederate Soldiers and Sailors Monument',\n",
       " 'Unknown',\n",
       " 'Madonna and Child',\n",
       " '',\n",
       " 'The Guide',\n",
       " 'Unknown',\n",
       " 'Unknown',\n",
       " 'Wagner',\n",
       " 'Unknown',\n",
       " '',\n",
       " '',\n",
       " 'Aegean Urn',\n",
       " 'Boanerges',\n",
       " 'Penguin',\n",
       " 'Greetings from Sowebo',\n",
       " '',\n",
       " '',\n",
       " 'Unknown',\n",
       " 'Frank Zappa',\n",
       " 'Martin Luther Monument',\n",
       " 'Untitled fountain',\n",
       " '',\n",
       " 'Citisphere',\n",
       " 'Unknown',\n",
       " 'Home Run 1984',\n",
       " '',\n",
       " 'Untitled',\n",
       " 'Unknown',\n",
       " 'Unknown',\n",
       " 'Johnny Unitas',\n",
       " 'George Peabody',\n",
       " 'King Penguin',\n",
       " '\"Children\\'s Round Square\"',\n",
       " 'May, 1940:The Destroyed City',\n",
       " 'Ram',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'An Ashanti (Ghana) Related Design',\n",
       " 'Unknown',\n",
       " 'Athena Parthenog',\n",
       " 'The Horse',\n",
       " '',\n",
       " 'Sod Buster/Chanticleer I\"',\n",
       " 'Unknown',\n",
       " '',\n",
       " 'The Dancers',\n",
       " 'Lady Baltimore',\n",
       " '',\n",
       " 'Boy Scout',\n",
       " 'The Discus Thrower',\n",
       " 'Unknown',\n",
       " '',\n",
       " 'Columbus monument',\n",
       " 'Samuel Smith Monument',\n",
       " '',\n",
       " 'Star Spangled Banner Centennial Monument',\n",
       " 'Locks, Keys, and Security Devices',\n",
       " 'Sea Birds',\n",
       " 'Unknown',\n",
       " 'Rogers Avenue',\n",
       " 'Unknown',\n",
       " 'Jacob France Fountain',\n",
       " \"Grasshopper's Revenge\",\n",
       " 'Balzac',\n",
       " 'Unknown',\n",
       " '',\n",
       " '',\n",
       " 'Peace Cross(Victory Cross)',\n",
       " 'Guilford Entrance: Antique Italian Well Head',\n",
       " '',\n",
       " 'Military Courage',\n",
       " '',\n",
       " 'Unknown',\n",
       " '',\n",
       " '',\n",
       " 'Unknown',\n",
       " 'Flagstaff',\n",
       " 'Endless Ribbon',\n",
       " 'Unknown',\n",
       " '',\n",
       " '',\n",
       " 'Unknown',\n",
       " 'Garrett Park Monument',\n",
       " '',\n",
       " 'Wells and McComas Monument',\n",
       " 'Hi-Fly',\n",
       " 'Mythological Horse',\n",
       " 'Untitled',\n",
       " 'Portrait of Frederick Douglass',\n",
       " '',\n",
       " 'Unknown',\n",
       " 'Faces',\n",
       " 'Equitable Trust Fountain',\n",
       " 'Unknown',\n",
       " 'Unknown',\n",
       " 'Propeller from the Battleship New Jersey',\n",
       " '',\n",
       " 'A Throne for the Eulipions',\n",
       " 'Untitled',\n",
       " 'Unknown',\n",
       " 'Roots of Life',\n",
       " 'Untitled',\n",
       " 'Memorial Stadium (Facade and urn)',\n",
       " 'Unknown',\n",
       " 'War Memorial: Aquatic Horses',\n",
       " 'Neon for Charles Street Station',\n",
       " 'Swung Over',\n",
       " 'Force',\n",
       " 'Order',\n",
       " 'Seated Lion',\n",
       " 'War',\n",
       " 'Unknown',\n",
       " 'Jazz Musicians',\n",
       " 'John Mifflen Hood',\n",
       " 'Alexander Brown & Four Sons',\n",
       " 'Boy and Turtle',\n",
       " 'Ground Play',\n",
       " 'Redwood Arch',\n",
       " 'Under Sky/One Family',\n",
       " 'Amanogawa',\n",
       " 'John Eager Howard Monument',\n",
       " 'The Battle Monument',\n",
       " 'Helios',\n",
       " 'The Briefing',\n",
       " 'The Right Light',\n",
       " 'Negro Soldier',\n",
       " \"Mayor Thomas D'Alesandro\",\n",
       " 'Severn Teackle Wallis',\n",
       " 'Streamings',\n",
       " 'The Diamond',\n",
       " 'Fan Figure',\n",
       " 'Lafayette Monument',\n",
       " 'Pride of Baltimore Memorial',\n",
       " 'Good News! Earth Is Dancing',\n",
       " 'Ecstasy of Love',\n",
       " 'Billie Holiday',\n",
       " 'Francis Scott Key Relief',\n",
       " 'Baltimore Federal',\n",
       " '\"Dolphin Fountain\"',\n",
       " 'Untitled (horse in pedestal)',\n",
       " 'Baltimore City Fraternal Order of Police Memorial',\n",
       " 'Great Ascension',\n",
       " 'Firebird',\n",
       " '',\n",
       " 'Birds Nest Balcony',\n",
       " 'Male/ Female',\n",
       " 'silence',\n",
       " 'Inertia Study']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles_of_artworks.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorize the Documents"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T03:31:32.389010Z",
     "start_time": "2021-10-29T03:31:32.374327Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#each row will be a document, each row will be a word in corpus\n",
    "vectorizer = CountVectorizer(stop_words = 'english')\n",
=======
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(stopwords=\"english\")\n",
>>>>>>> 49893f1efd88573d5690756d970064fdaa6b5eec
    "X = vectorizer.fit_transform(titles_of_artworks) \n",
    "X = X.toarray()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T03:31:33.952877Z",
     "start_time": "2021-10-29T03:31:33.920340Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['10',\n",
       " '1890',\n",
       " '1912',\n",
       " '1940',\n",
       " '1984',\n",
       " '31',\n",
       " '420',\n",
       " '43',\n",
       " 'aegean',\n",
       " 'african',\n",
       " 'aircraft',\n",
       " 'albermarle',\n",
       " 'alesandro',\n",
       " 'alexander',\n",
       " 'allan',\n",
       " 'amanogawa',\n",
       " 'american',\n",
       " 'angel',\n",
       " 'animals',\n",
       " 'antique',\n",
       " 'apollo',\n",
       " 'aquatic',\n",
       " 'arch',\n",
       " 'archimedean',\n",
       " 'area',\n",
       " 'ark',\n",
       " 'armistead',\n",
       " 'armory',\n",
       " 'art',\n",
       " 'arts',\n",
       " 'ascension',\n",
       " 'ashanti',\n",
       " 'athena',\n",
       " 'atlantic',\n",
       " 'avenue',\n",
       " 'babe',\n",
       " 'baby',\n",
       " 'bactrian',\n",
       " 'bad',\n",
       " 'balcony',\n",
       " 'baltimore',\n",
       " 'baltimores',\n",
       " 'balzac',\n",
       " 'bank',\n",
       " 'banner',\n",
       " 'baptist',\n",
       " 'bas',\n",
       " 'bastion',\n",
       " 'bather',\n",
       " 'battery',\n",
       " 'battle',\n",
       " 'battleship',\n",
       " 'bear',\n",
       " 'believe',\n",
       " 'bell',\n",
       " 'belvedere',\n",
       " 'berg',\n",
       " 'biennial',\n",
       " 'billie',\n",
       " 'bird',\n",
       " 'birds',\n",
       " 'birth',\n",
       " 'blue',\n",
       " 'bluebird',\n",
       " 'bma',\n",
       " 'boanerges',\n",
       " 'board',\n",
       " 'boat',\n",
       " 'bolivar',\n",
       " 'bolt',\n",
       " 'bound',\n",
       " 'bowman',\n",
       " 'box',\n",
       " 'boy',\n",
       " 'bridge',\n",
       " 'briefing',\n",
       " 'brio',\n",
       " 'brooks',\n",
       " 'brown',\n",
       " 'building',\n",
       " 'bulletin',\n",
       " 'buoyant',\n",
       " 'bust',\n",
       " 'buster',\n",
       " 'butterflies',\n",
       " 'calvert',\n",
       " 'camel',\n",
       " 'cannonball',\n",
       " 'cannons',\n",
       " 'canton',\n",
       " 'captain',\n",
       " 'cardinal',\n",
       " 'care',\n",
       " 'cascade',\n",
       " 'cat',\n",
       " 'caterpillar',\n",
       " 'cathedral',\n",
       " 'cecilius',\n",
       " 'centennial',\n",
       " 'ceramic',\n",
       " 'chair',\n",
       " 'chamber',\n",
       " 'chance',\n",
       " 'chanticleer',\n",
       " 'chapin',\n",
       " 'chariot',\n",
       " 'charles',\n",
       " 'child',\n",
       " 'children',\n",
       " 'chorale',\n",
       " 'christ',\n",
       " 'christopher',\n",
       " 'citisphere',\n",
       " 'city',\n",
       " 'clock',\n",
       " 'coit',\n",
       " 'cold',\n",
       " 'colonel',\n",
       " 'colors',\n",
       " 'colossus',\n",
       " 'columbus',\n",
       " 'column',\n",
       " 'commerce',\n",
       " 'company',\n",
       " 'conception',\n",
       " 'confederate',\n",
       " 'conradin',\n",
       " 'constant',\n",
       " 'contortionist',\n",
       " 'cook',\n",
       " 'copy',\n",
       " 'courage',\n",
       " 'course',\n",
       " 'courthouse',\n",
       " 'covered',\n",
       " 'creator',\n",
       " 'cross',\n",
       " 'cubs',\n",
       " 'cultura',\n",
       " 'culture',\n",
       " 'curran',\n",
       " 'cylburn',\n",
       " 'dance',\n",
       " 'dancers',\n",
       " 'dancing',\n",
       " 'daniel',\n",
       " 'department',\n",
       " 'design',\n",
       " 'destroyed',\n",
       " 'devices',\n",
       " 'dhontshihgwa',\n",
       " 'dial',\n",
       " 'diamond',\n",
       " 'discus',\n",
       " 'dishes',\n",
       " 'divine',\n",
       " 'dolphin',\n",
       " 'donald',\n",
       " 'donnell',\n",
       " 'dos',\n",
       " 'double',\n",
       " 'douglass',\n",
       " 'dream',\n",
       " 'dreams',\n",
       " 'drive',\n",
       " 'dromedary',\n",
       " 'eager',\n",
       " 'eagles',\n",
       " 'earth',\n",
       " 'easy',\n",
       " 'ecstasy',\n",
       " 'edgar',\n",
       " 'education',\n",
       " 'edward',\n",
       " 'electric',\n",
       " 'elephant',\n",
       " 'elevation',\n",
       " 'ellipse',\n",
       " 'endless',\n",
       " 'energy',\n",
       " 'engine',\n",
       " 'enlargement',\n",
       " 'entrance',\n",
       " 'equitable',\n",
       " 'es',\n",
       " 'eulipions',\n",
       " 'exhibition',\n",
       " 'facade',\n",
       " 'faces',\n",
       " 'fallsway',\n",
       " 'family',\n",
       " 'fan',\n",
       " 'fanned',\n",
       " 'father',\n",
       " 'fa√ßade',\n",
       " 'federal',\n",
       " 'fell',\n",
       " 'female',\n",
       " 'fifth',\n",
       " 'fighters',\n",
       " 'figure',\n",
       " 'fine',\n",
       " 'firebird',\n",
       " 'fireman',\n",
       " 'fish',\n",
       " 'flagstaff',\n",
       " 'flame',\n",
       " 'flower',\n",
       " 'fly',\n",
       " 'flying',\n",
       " 'force',\n",
       " 'foreign',\n",
       " 'form',\n",
       " 'forum',\n",
       " 'fountain',\n",
       " 'fountains',\n",
       " 'france',\n",
       " 'francis',\n",
       " 'frank',\n",
       " 'fraternal',\n",
       " 'frederick',\n",
       " 'freedom',\n",
       " 'friends',\n",
       " 'frieze',\n",
       " 'fruit',\n",
       " 'gallagher',\n",
       " 'game',\n",
       " 'ganut',\n",
       " 'garden',\n",
       " 'garrett',\n",
       " 'gas',\n",
       " 'george',\n",
       " 'ghana',\n",
       " 'gibbons',\n",
       " 'gilman',\n",
       " 'giraffe',\n",
       " 'girl',\n",
       " 'good',\n",
       " 'goose',\n",
       " 'granite',\n",
       " 'grasshopper',\n",
       " 'great',\n",
       " 'greetings',\n",
       " 'ground',\n",
       " 'group',\n",
       " 'grove',\n",
       " 'growth',\n",
       " 'guide',\n",
       " 'guilford',\n",
       " 'hall',\n",
       " 'happiness',\n",
       " 'harbor',\n",
       " 'harmony',\n",
       " 'harpie',\n",
       " 'harris',\n",
       " 'head',\n",
       " 'headed',\n",
       " 'healer',\n",
       " 'helios',\n",
       " 'help',\n",
       " 'henge',\n",
       " 'hi',\n",
       " 'hiker',\n",
       " 'holiday',\n",
       " 'holocaust',\n",
       " 'home',\n",
       " 'honor',\n",
       " 'hood',\n",
       " 'hopkins',\n",
       " 'horse',\n",
       " 'horses',\n",
       " 'host',\n",
       " 'house',\n",
       " 'howard',\n",
       " 'human',\n",
       " 'ii',\n",
       " 'inertia',\n",
       " 'interlocking',\n",
       " 'isaiah',\n",
       " 'italian',\n",
       " 'iv',\n",
       " 'jackson',\n",
       " 'jacob',\n",
       " 'james',\n",
       " 'jazz',\n",
       " 'jefferson',\n",
       " 'jersey',\n",
       " 'job',\n",
       " 'john',\n",
       " 'johnny',\n",
       " 'johns',\n",
       " 'joseph',\n",
       " 'joy',\n",
       " 'key',\n",
       " 'keys',\n",
       " 'king',\n",
       " 'kinship',\n",
       " 'knot',\n",
       " 'knowledge',\n",
       " 'korean',\n",
       " 'kreutzer',\n",
       " 'lady',\n",
       " 'lafayette',\n",
       " 'lake',\n",
       " 'lakeland',\n",
       " 'landing',\n",
       " 'lanier',\n",
       " 'lantern',\n",
       " 'large',\n",
       " 'latrobe',\n",
       " 'leaden',\n",
       " 'leaders',\n",
       " 'learning',\n",
       " 'lee',\n",
       " 'liberty',\n",
       " 'life',\n",
       " 'light',\n",
       " 'line',\n",
       " 'linear',\n",
       " 'link',\n",
       " 'lion',\n",
       " 'lions',\n",
       " 'little',\n",
       " 'lo',\n",
       " 'local',\n",
       " 'locks',\n",
       " 'love',\n",
       " 'luther',\n",
       " 'madonna',\n",
       " 'major',\n",
       " 'male',\n",
       " 'man',\n",
       " 'mark',\n",
       " 'market',\n",
       " 'marshall',\n",
       " 'martin',\n",
       " 'mary',\n",
       " 'maryland',\n",
       " 'mayor',\n",
       " 'mccomas',\n",
       " 'mckeldin',\n",
       " 'medal',\n",
       " 'memorial',\n",
       " 'memory',\n",
       " 'mencken',\n",
       " 'mercury',\n",
       " 'mifflen',\n",
       " 'military',\n",
       " 'miller',\n",
       " 'minor',\n",
       " 'miracle',\n",
       " 'monlith',\n",
       " 'monument',\n",
       " 'moon',\n",
       " 'moses',\n",
       " 'mother',\n",
       " 'mr',\n",
       " 'musem',\n",
       " 'museum',\n",
       " 'musicians',\n",
       " 'mythological',\n",
       " 'naiad',\n",
       " 'negro',\n",
       " 'neon',\n",
       " 'nest',\n",
       " 'new',\n",
       " 'news',\n",
       " 'noah',\n",
       " 'north',\n",
       " 'number',\n",
       " 'nut',\n",
       " 'oasis',\n",
       " 'obelisk',\n",
       " 'office',\n",
       " 'old',\n",
       " 'om',\n",
       " 'order',\n",
       " 'original',\n",
       " 'orpheus',\n",
       " 'otters',\n",
       " 'outcrop',\n",
       " 'owl',\n",
       " 'paestum',\n",
       " 'palace',\n",
       " 'park',\n",
       " 'parthenog',\n",
       " 'pasarelle',\n",
       " 'passage',\n",
       " 'patapsco',\n",
       " 'peabody',\n",
       " 'peace',\n",
       " 'peale',\n",
       " 'pearlstone',\n",
       " 'pedestal',\n",
       " 'pegasus',\n",
       " 'penguin',\n",
       " 'piece',\n",
       " 'play',\n",
       " 'playfield',\n",
       " 'playground',\n",
       " 'plaza',\n",
       " 'poe',\n",
       " 'point',\n",
       " 'points',\n",
       " 'police',\n",
       " 'portrait',\n",
       " 'portraits',\n",
       " 'post',\n",
       " 'prayer',\n",
       " 'pride',\n",
       " 'project',\n",
       " 'prometheus',\n",
       " 'propeller',\n",
       " 'prophet',\n",
       " 'pulaski',\n",
       " 'pylon',\n",
       " 'queen',\n",
       " 'quest',\n",
       " 'question',\n",
       " 'rack',\n",
       " 'ram',\n",
       " 'reawakening',\n",
       " 'reclining',\n",
       " 'recreation',\n",
       " 'red',\n",
       " 'redwood',\n",
       " 'reese',\n",
       " 'regiment',\n",
       " 'related',\n",
       " 'relay',\n",
       " 'releasing',\n",
       " 'relief',\n",
       " 'reliefs',\n",
       " 'rememberance',\n",
       " 'repeal',\n",
       " 'rescue',\n",
       " 'revenge',\n",
       " 'ribbon',\n",
       " 'ridgely',\n",
       " 'rifleman',\n",
       " 'right',\n",
       " 'rise',\n",
       " 'river',\n",
       " 'robinson',\n",
       " 'rocker',\n",
       " 'roger',\n",
       " 'rogers',\n",
       " 'roller',\n",
       " 'roots',\n",
       " 'round',\n",
       " 'run',\n",
       " 'runner',\n",
       " 'ryan',\n",
       " 'sailor',\n",
       " 'sailors',\n",
       " 'sails',\n",
       " 'saint',\n",
       " 'samuel',\n",
       " 'samurai',\n",
       " 'sanctuary',\n",
       " 'sandtown',\n",
       " 'saving',\n",
       " 'schaefer',\n",
       " 'scott',\n",
       " 'scout',\n",
       " 'sculpture',\n",
       " 'sculptures',\n",
       " 'sea',\n",
       " 'seal',\n",
       " 'seasons',\n",
       " 'seated',\n",
       " 'security',\n",
       " 'seeds',\n",
       " 'seen',\n",
       " 'series',\n",
       " 'serviceman',\n",
       " 'severn',\n",
       " 'shaft',\n",
       " 'shelter',\n",
       " 'sidney',\n",
       " 'silence',\n",
       " 'simon',\n",
       " 'single',\n",
       " 'sister',\n",
       " 'sisyphus',\n",
       " 'sky',\n",
       " 'smith',\n",
       " 'snail',\n",
       " 'sod',\n",
       " 'solar',\n",
       " 'soldier',\n",
       " 'soldiers',\n",
       " 'song',\n",
       " 'sons',\n",
       " 'sound',\n",
       " 'sowebo',\n",
       " 'soying',\n",
       " 'spangled',\n",
       " 'spanish',\n",
       " 'spiral',\n",
       " 'spirit',\n",
       " 'spring',\n",
       " 'square',\n",
       " 'squre',\n",
       " 'st',\n",
       " 'stadium',\n",
       " 'standing',\n",
       " 'star',\n",
       " 'station',\n",
       " 'statue',\n",
       " 'steel',\n",
       " 'stones',\n",
       " 'strangling',\n",
       " 'streamings',\n",
       " 'street',\n",
       " 'structure',\n",
       " 'strugg',\n",
       " 'study',\n",
       " 'subvator',\n",
       " 'sun',\n",
       " 'sundial',\n",
       " 'suspended',\n",
       " 'swung',\n",
       " 'tablet',\n",
       " 'tachigata',\n",
       " 'taney',\n",
       " 'tauromachy',\n",
       " 'teackle',\n",
       " 'thomas',\n",
       " 'throne',\n",
       " 'thrower',\n",
       " 'thurgood',\n",
       " 'titan',\n",
       " 'totum',\n",
       " 'trail',\n",
       " 'tree',\n",
       " 'triaxial',\n",
       " 'tribute',\n",
       " 'trust',\n",
       " 'truth',\n",
       " 'turning',\n",
       " 'turtle',\n",
       " 'uni',\n",
       " 'union',\n",
       " 'unitas',\n",
       " 'unity',\n",
       " 'unknown',\n",
       " 'uno',\n",
       " 'untitled',\n",
       " 'urchin',\n",
       " 'urn',\n",
       " 'ursa',\n",
       " 'various',\n",
       " 'venter',\n",
       " 'venus',\n",
       " 'veronica',\n",
       " 'veteran',\n",
       " 'veterans',\n",
       " 'victory',\n",
       " 'vietnam',\n",
       " 'vine',\n",
       " 'visitation',\n",
       " 'wagner',\n",
       " 'walking',\n",
       " 'wall',\n",
       " 'wallace',\n",
       " 'wallis',\n",
       " 'walters',\n",
       " 'war',\n",
       " 'wari',\n",
       " 'wars',\n",
       " 'washington',\n",
       " 'watson',\n",
       " 'welch',\n",
       " 'wells',\n",
       " 'westside',\n",
       " 'wild',\n",
       " 'wildey',\n",
       " 'wildflower',\n",
       " 'william',\n",
       " 'wishbone',\n",
       " 'woman',\n",
       " 'women',\n",
       " 'world',\n",
       " 'young',\n",
       " 'yuai',\n",
       " 'yum',\n",
       " 'zappa']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
=======
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
>>>>>>> 49893f1efd88573d5690756d970064fdaa6b5eec
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T03:27:41.758786Z",
     "start_time": "2021-10-29T03:27:41.735289Z"
    }
   },
=======
   "execution_count": 30,
   "metadata": {},
>>>>>>> 49893f1efd88573d5690756d970064fdaa6b5eec
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10</th>\n",
       "      <th>1890</th>\n",
       "      <th>1912</th>\n",
       "      <th>1940</th>\n",
       "      <th>1984</th>\n",
       "      <th>31</th>\n",
       "      <th>420</th>\n",
       "      <th>43</th>\n",
       "      <th>aegean</th>\n",
       "      <th>african</th>\n",
       "      <th>...</th>\n",
<<<<<<< HEAD
       "      <th>william</th>\n",
       "      <th>wishbone</th>\n",
       "      <th>with</th>\n",
=======
       "      <th>wildflower</th>\n",
       "      <th>william</th>\n",
       "      <th>wishbone</th>\n",
>>>>>>> 49893f1efd88573d5690756d970064fdaa6b5eec
       "      <th>woman</th>\n",
       "      <th>women</th>\n",
       "      <th>world</th>\n",
       "      <th>young</th>\n",
       "      <th>yuai</th>\n",
       "      <th>yum</th>\n",
       "      <th>zappa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
<<<<<<< HEAD
       "      <td>1</td>\n",
       "      <td>0</td>\n",
=======
       "      <td>0</td>\n",
       "      <td>1</td>\n",
>>>>>>> 49893f1efd88573d5690756d970064fdaa6b5eec
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>685</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>686</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>687</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>688</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>689</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
<<<<<<< HEAD
       "<p>690 rows √ó 619 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     10  1890  1912  1940  1984  31  420  43  aegean  african  ...  william  \\\n",
       "0     0     0     0     0     0   0    0   0       0        0  ...        0   \n",
       "1     0     0     0     0     0   0    0   0       0        0  ...        0   \n",
       "2     0     0     0     0     0   0    0   0       0        0  ...        1   \n",
       "3     0     0     0     0     0   0    0   0       0        0  ...        0   \n",
       "4     0     0     0     0     0   0    0   0       0        0  ...        0   \n",
       "..   ..   ...   ...   ...   ...  ..  ...  ..     ...      ...  ...      ...   \n",
       "685   0     0     0     0     0   0    0   0       0        0  ...        0   \n",
       "686   0     0     0     0     0   0    0   0       0        0  ...        0   \n",
       "687   0     0     0     0     0   0    0   0       0        0  ...        0   \n",
       "688   0     0     0     0     0   0    0   0       0        0  ...        0   \n",
       "689   0     0     0     0     0   0    0   0       0        0  ...        0   \n",
       "\n",
       "     wishbone  with  woman  women  world  young  yuai  yum  zappa  \n",
       "0           0     0      0      0      0      0     0    0      0  \n",
       "1           0     0      0      0      0      0     0    0      0  \n",
       "2           0     0      0      0      0      0     0    0      0  \n",
       "3           0     0      0      0      0      0     0    0      0  \n",
       "4           0     0      0      0      0      0     0    0      0  \n",
       "..        ...   ...    ...    ...    ...    ...   ...  ...    ...  \n",
       "685         0     0      0      0      0      0     0    0      0  \n",
       "686         0     0      0      0      0      0     0    0      0  \n",
       "687         0     0      0      0      0      0     0    0      0  \n",
       "688         0     0      0      0      0      0     0    0      0  \n",
       "689         0     0      0      0      0      0     0    0      0  \n",
       "\n",
       "[690 rows x 619 columns]"
      ]
     },
     "execution_count": 53,
=======
       "<p>690 rows √ó 597 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     10  1890  1912  1940  1984  31  420  43  aegean  african  ...  \\\n",
       "0     0     0     0     0     0   0    0   0       0        0  ...   \n",
       "1     0     0     0     0     0   0    0   0       0        0  ...   \n",
       "2     0     0     0     0     0   0    0   0       0        0  ...   \n",
       "3     0     0     0     0     0   0    0   0       0        0  ...   \n",
       "4     0     0     0     0     0   0    0   0       0        0  ...   \n",
       "..   ..   ...   ...   ...   ...  ..  ...  ..     ...      ...  ...   \n",
       "685   0     0     0     0     0   0    0   0       0        0  ...   \n",
       "686   0     0     0     0     0   0    0   0       0        0  ...   \n",
       "687   0     0     0     0     0   0    0   0       0        0  ...   \n",
       "688   0     0     0     0     0   0    0   0       0        0  ...   \n",
       "689   0     0     0     0     0   0    0   0       0        0  ...   \n",
       "\n",
       "     wildflower  william  wishbone  woman  women  world  young  yuai  yum  \\\n",
       "0             0        0         0      0      0      0      0     0    0   \n",
       "1             0        0         0      0      0      0      0     0    0   \n",
       "2             0        1         0      0      0      0      0     0    0   \n",
       "3             0        0         0      0      0      0      0     0    0   \n",
       "4             0        0         0      0      0      0      0     0    0   \n",
       "..          ...      ...       ...    ...    ...    ...    ...   ...  ...   \n",
       "685           0        0         0      0      0      0      0     0    0   \n",
       "686           0        0         0      0      0      0      0     0    0   \n",
       "687           0        0         0      0      0      0      0     0    0   \n",
       "688           0        0         0      0      0      0      0     0    0   \n",
       "689           0        0         0      0      0      0      0     0    0   \n",
       "\n",
       "     zappa  \n",
       "0        0  \n",
       "1        0  \n",
       "2        0  \n",
       "3        0  \n",
       "4        0  \n",
       "..     ...  \n",
       "685      0  \n",
       "686      0  \n",
       "687      0  \n",
       "688      0  \n",
       "689      0  \n",
       "\n",
       "[690 rows x 597 columns]"
      ]
     },
     "execution_count": 30,
>>>>>>> 49893f1efd88573d5690756d970064fdaa6b5eec
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
<<<<<<< HEAD
    "corpus_df = pd.DataFrame(X, columns=vectorizer.get_feature_names())\n",
=======
>>>>>>> 49893f1efd88573d5690756d970064fdaa6b5eec
    "corpus_df"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T03:28:34.707204Z",
     "start_time": "2021-10-29T03:28:34.702361Z"
    }
   },
=======
   "execution_count": 28,
   "metadata": {},
>>>>>>> 49893f1efd88573d5690756d970064fdaa6b5eec
   "outputs": [
    {
     "data": {
      "text/plain": [
<<<<<<< HEAD
       "'William Donald Schaefer'"
      ]
     },
     "execution_count": 56,
=======
       "{'an',\n",
       " 'and',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'from',\n",
       " 'in',\n",
       " 'is',\n",
       " 'my',\n",
       " 'no',\n",
       " 'of',\n",
       " 'on',\n",
       " 'or',\n",
       " 'our',\n",
       " 'over',\n",
       " 'the',\n",
       " 'to',\n",
       " 'under',\n",
       " 'we',\n",
       " 'what',\n",
       " 'will',\n",
       " 'with'}"
      ]
     },
     "execution_count": 28,
>>>>>>> 49893f1efd88573d5690756d970064fdaa6b5eec
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
<<<<<<< HEAD
    "titles_of_artworks[2]\n",
    "#william is in the title!"
=======
    "# iterate through the Pandas dataframe, and drop the columns that reflect stopwords:\n",
    "original_columns = corpus_df.columns # get existing columns\n",
    "\n",
    "to_drop_columns = set(original_columns).intersection(set(stopwords)) # get the list of words to drop\n",
    "to_drop_columns"
>>>>>>> 49893f1efd88573d5690756d970064fdaa6b5eec
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T03:30:39.440923Z",
     "start_time": "2021-10-29T03:30:39.415376Z"
    }
   },
=======
=======
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'like'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatizer.lemmatize(\"liked\", pos=\"v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'liked'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatizer.lemmatize(\"liked\") # assumes this is a noun"
   ]
  },
  {
   "cell_type": "code",
>>>>>>> 3a0dbf42123cdd124888bd42e474cd56c428990d
   "execution_count": 29,
   "metadata": {},
>>>>>>> 49893f1efd88573d5690756d970064fdaa6b5eec
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe shape was (690, 619)\n",
      "Dataframe shape is now(690, 597)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dataframe shape was {corpus_df.shape}\")\n",
    "corpus_df.drop(columns=to_drop_columns, inplace=True)\n",
    "print(f\"Dataframe shape is now{corpus_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When Should You Avoid Removing Stopwords?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below example is taken from a Medium article by Gagandeep Singh, [\"Why you should avoid removing STOPWORDS\"](https://towardsdatascience.com/why-you-should-avoid-removing-stopwords-aa7a353d2a52)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T03:24:14.223709Z",
     "start_time": "2021-10-29T03:24:14.219782Z"
    }
   },
   "outputs": [],
   "source": [
    "reviews = [\n",
    "    (\"The product is really very good\", \"Positive\"),\n",
    "    (\"The products seems to be good\", \"Positive\"),\n",
    "    (\"Good product. I really liked it\", \"Positive\"),\n",
    "    (\"I didn‚Äôt like the product\", \"Negative\"),\n",
    "    (\"The product is not good.\", \"Negative\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T03:24:14.612362Z",
     "start_time": "2021-10-29T03:24:14.604569Z"
    }
   },
   "outputs": [],
   "source": [
    "cleaned_reviews = []\n",
    "nltk_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "# This iterates through each of the reviews, splitting the review into distinct tokens\n",
    "# Then it checks each token for whether or not it is a stopword, before adding them back into a \"cleaned_review\"\n",
    "for review in reviews:\n",
    "    words = nltk.word_tokenize(review[0])\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.lower() in nltk_stopwords:\n",
    "            continue\n",
    "        new_words.append(word)\n",
    "    cleaned_review = \" \".join(new_words)\n",
    "    cleaned_reviews.append((cleaned_review, review[1]))"
   ]
  },
  {
<<<<<<< HEAD
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T03:24:16.342344Z",
     "start_time": "2021-10-29T03:24:16.332611Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The product really good', 'Positive'),\n",
       " ('The products seems good', 'Positive'),\n",
       " ('Good product . I really liked', 'Positive'),\n",
       " ('I ‚Äô like product', 'Negative'),\n",
       " ('The product good .', 'Negative')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_reviews"
   ]
  },
  {
=======
>>>>>>> 49893f1efd88573d5690756d970064fdaa6b5eec
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Singh:\n",
    "> If you are working with **basic NLP techniques like BOW, Count Vectorizer or TF-IDF(Term Frequency and Inverse Document Frequency)** then removing stopwords is a good idea because stopwords act like noise for these methods. If you working with **LSTMs or other models which capture the semantic meaning and the meaning of a word depends on the context of the previous text**, then it becomes important not to remove stopwords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise:\n",
    "\n",
    "##### 1. For each of the following statements, label them True or False. If False, briefly explain why:\n",
    "\n",
    "A. *Text typically should be processed via either stemming or lemmatization, but not both.*\n",
    "\n",
    "B. *Texts processed using lemmatization will typically have higher recall than stemming.*\n",
    "\n",
    "C. *If the **F1 score** of a model is **1.0 (100%)**, then the accuracy of your model must also be **100%**.*\n",
    "\n",
    "D. *Stemming **increases the size of the vocabulary** (the vocabulary is the set of all tokens found inside the corpus)*\n",
    "\n",
    "##### 2. Calculate precison and recall given the following results from a confusion matrix:\n",
    "\n",
    "<img src=\"images/exercise.jpeg\" alt=\"Different Stemming Techniques\" style=\"width:600px;\"/>\n",
    "\n",
    "##### 3. Provide an example of how stemming can improve recall.\n",
    "\n",
    "##### 4. Provide an example of when stemming might reduce precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Vectorization\n",
    "<img src=\"images/count_vectorizer.png\" alt=\"Different Stemming Techniques\" style=\"width:600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use **`sklearn.feature_extraction.text.CountVectorizer`** to easily convert your corpus into a bag of words matrix:\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "data_corpus = [\"John likes to watch movies. Mary likes movies too.\", \n",
    "\"John also likes to watch football games. Mary does not like football much.\"]\n",
    "X = vectorizer.fit_transform(data_corpus) \n",
    "```\n",
    "Note that the output `X` here is not your traditional Numpy matrix! Calling **`type(X)`** here will yield **`<class 'scipy.sparse.csr.csr_matrix'>`**, which is a **CSR ([compressed sparse row format matrix](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.sparse.csr_matrix.html))**. To convert it into an actual matrix, call the `toarray()` method:\n",
    "\n",
    "```python\n",
    "X.toarray()\n",
    "```\n",
    "Your output will be \n",
    "\n",
    "```\n",
    "array([[0, 0, 0, 0, 1, 0, 2, 1, 2, 0, 0, 1, 1, 1],\n",
    "       [1, 1, 2, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1]], dtype=int64)\n",
    "```\n",
    "Notice that using **`X.shape`** $\\rightarrow$ `(2,14)`, indicating a total vocabulary size $V$ of 14. To get what word each of the 14 columns corresponds to, use **`vectorizer.get_feature_names()`**:\n",
    "```\n",
    "['also', 'does', 'football', 'games', 'john', 'like', 'likes', 'mary', 'movies', 'much', 'not', 'to', 'too', 'watch']\n",
    "```\n",
    "\n",
    "Notice, however, that as the vocabulary size $V$ increases, the percent of the matrix taken up by zero values increases:\n",
    "\n",
    "```python\n",
    "corpus = [\n",
    "    \"Some analysts think demand could drop this year because a large number of homeowners take on remodeling projectsafter buying a new property. With fewer homes selling, home values easing, and mortgage rates rising, they predict home renovations could fall to their lowest levels in three years.\", \n",
    "    \n",
    "          \"Most home improvement stocks are expected to report fourth-quarter earnings next month.\",\n",
    "    \n",
    "         \"The conversation boils down to how much leverage management can get out of its wide-ranging efforts to re-energize operations, branding, digital capabilities, and the menu‚Äìand, for investors, how much to pay for that.\",\n",
    "    \n",
    "    \"RMD‚Äôs software acquisitions, efficiency, and mix overcame pricing and its gross margin improved by 90 bps Y/Y while its operating margin (including amortization) improved by 80 bps Y/Y. Since RMD expects the slower international flow generator growth to continue for the next few quarters, we have lowered our organic growth estimates to the mid-single digits.\"\n",
    "]\n",
    "\n",
    "X = vectorizer.fit_transform(corpus).toarray() \n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T03:39:43.545454Z",
     "start_time": "2021-10-29T03:39:43.538776Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 59)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# list of text documents\n",
    "text = [\"It's still early, so box-office disappointments are still among the highest-grossing movies of the year.\", \n",
    "        \"That movie was terrific\", \"You love cats\", \n",
    "        \"Pay for top executives at big US companies is vastly higher than what everyday workers make, and a new report from The Wall Street Journal has found that CEOs have hit an eye-popping milestone in the size of their monthly paychecks.\"]\n",
    "# create the transform\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(text)\n",
    "\n",
    "# vectorize the corpus\n",
    "vector = vectorizer.transform(text)\n",
    "\n",
    "# summarize encoded vector\n",
    "print(vector.shape)\n",
    "\n",
    "# Notice what type of object this is\n",
    "print(type(vector))"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T03:39:54.539636Z",
     "start_time": "2021-10-29T03:39:54.532996Z"
    }
   },
=======
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1],\n",
       "       [1, 1]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "docs = [\"I love cats\", \"cats love I\"]\n",
    "\n",
    "vectorizer.fit_transform(docs).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
>>>>>>> 49893f1efd88573d5690756d970064fdaa6b5eec
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 1 0 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 1\n",
      "  1 0 0 0 0 0 1 2 0 0 0 0 2 0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [0 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 1 1 0 1 1 1 0 1 1 1 0 1 0 1 1 1 0 0 1 1\n",
      "  0 1 1 1 1 1 0 0 1 0 1 1 2 1 1 1 1 1 0 1 1 0 0]]\n",
      "['among', 'an', 'and', 'are', 'at', 'big', 'box', 'cats', 'ceos', 'companies', 'disappointments', 'early', 'everyday', 'executives', 'eye', 'for', 'found', 'from', 'grossing', 'has', 'have', 'higher', 'highest', 'hit', 'in', 'is', 'it', 'journal', 'love', 'make', 'milestone', 'monthly', 'movie', 'movies', 'new', 'of', 'office', 'pay', 'paychecks', 'popping', 'report', 'size', 'so', 'still', 'street', 'terrific', 'than', 'that', 'the', 'their', 'top', 'us', 'vastly', 'wall', 'was', 'what', 'workers', 'year', 'you']\n"
     ]
    }
   ],
   "source": [
    "# see the outputted vectors\n",
    "print(vector.toarray())\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>among</th>\n",
       "      <th>an</th>\n",
       "      <th>and</th>\n",
       "      <th>are</th>\n",
       "      <th>at</th>\n",
       "      <th>big</th>\n",
       "      <th>box</th>\n",
       "      <th>cats</th>\n",
       "      <th>ceos</th>\n",
       "      <th>companies</th>\n",
       "      <th>...</th>\n",
       "      <th>their</th>\n",
       "      <th>top</th>\n",
       "      <th>us</th>\n",
       "      <th>vastly</th>\n",
       "      <th>wall</th>\n",
       "      <th>was</th>\n",
       "      <th>what</th>\n",
       "      <th>workers</th>\n",
       "      <th>year</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>...</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>...</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows √ó 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       among    an   and   are    at   big   box  cats  ceos  companies  ...  \\\n",
       "count   4.00  4.00  4.00  4.00  4.00  4.00  4.00  4.00  4.00       4.00  ...   \n",
       "mean    0.25  0.25  0.25  0.25  0.25  0.25  0.25  0.25  0.25       0.25  ...   \n",
       "std     0.50  0.50  0.50  0.50  0.50  0.50  0.50  0.50  0.50       0.50  ...   \n",
       "min     0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00       0.00  ...   \n",
       "25%     0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00       0.00  ...   \n",
       "50%     0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00       0.00  ...   \n",
       "75%     0.25  0.25  0.25  0.25  0.25  0.25  0.25  0.25  0.25       0.25  ...   \n",
       "max     1.00  1.00  1.00  1.00  1.00  1.00  1.00  1.00  1.00       1.00  ...   \n",
       "\n",
       "       their   top    us  vastly  wall   was  what  workers  year   you  \n",
       "count   4.00  4.00  4.00    4.00  4.00  4.00  4.00     4.00  4.00  4.00  \n",
       "mean    0.25  0.25  0.25    0.25  0.25  0.25  0.25     0.25  0.25  0.25  \n",
       "std     0.50  0.50  0.50    0.50  0.50  0.50  0.50     0.50  0.50  0.50  \n",
       "min     0.00  0.00  0.00    0.00  0.00  0.00  0.00     0.00  0.00  0.00  \n",
       "25%     0.00  0.00  0.00    0.00  0.00  0.00  0.00     0.00  0.00  0.00  \n",
       "50%     0.00  0.00  0.00    0.00  0.00  0.00  0.00     0.00  0.00  0.00  \n",
       "75%     0.25  0.25  0.25    0.25  0.25  0.25  0.25     0.25  0.25  0.25  \n",
       "max     1.00  1.00  1.00    1.00  1.00  1.00  1.00     1.00  1.00  1.00  \n",
       "\n",
       "[8 rows x 59 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load vectorized corpus into Pandas dataframe\n",
    "import pandas as pd\n",
    "corpus_df = pd.DataFrame(vector.toarray(), columns=vectorizer.get_feature_names())\n",
    "corpus_df.describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "319px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
